{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comp 551 - Assignment 1\n",
    "## Taha Salman (260721174)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Sampling\n",
    "#### Question 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sampling distribution for the grad student's routine for 100 days is:\n",
      "0.14 of these days were spent watching movies\n",
      "0.43 of these days were spent doing COMP 551\n",
      "0.15 of these days were spent playing\n",
      "0.28 of these days were spent studying\n",
      "_______________________________________\n",
      "A sampling distribution for the grad student's routine for 1000 days is:\n",
      "0.206 of these days were spent watching movies\n",
      "0.4 of these days were spent doing COMP 551\n",
      "0.094 of these days were spent playing\n",
      "0.3 of these days were spent studying\n",
      "_______________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "(Question 2)\n",
    "\n",
    "This program implements the pseudocode written down in question 1 and finds the sampling distribution for the grad\n",
    "student specified.\n",
    "'''\n",
    "from random import random\n",
    "\n",
    "def pick_activity():\n",
    "    '''\n",
    "    This method randomly picks an activity for the given day based on the given probabilities.\n",
    "    '''\n",
    "    random_num = random()\n",
    "    if random_num < 0.2:\n",
    "        return \"M\"\n",
    "    elif random_num < 0.6:\n",
    "        return \"C\"\n",
    "    elif random_num < 0.7:\n",
    "        return \"P\"\n",
    "    else:\n",
    "        return \"S\"\n",
    "\n",
    "def simulate_days(num_days):\n",
    "    '''\n",
    "    This method simulates the number of days specified to create a sample distribution for the grad student's schedule.\n",
    "    '''\n",
    "    count = 0\n",
    "    '''\n",
    "    This variable stores a record of the number of times an activity was done during the specified time interval. Each\n",
    "    activity is represented by a character which is the same as its first letter capitalized.\n",
    "    '''\n",
    "    activities_counter = {      \n",
    "        \"M\": 0,\n",
    "        \"C\": 0,\n",
    "        \"P\": 0,\n",
    "        \"S\": 0\n",
    "    }\n",
    "\n",
    "    while count<num_days:\n",
    "        activity = pick_activity()\n",
    "        activities_counter[activity] += 1\n",
    "        count+=1\n",
    "\n",
    "    distribution = {}\n",
    "\n",
    "    for activity in activities_counter:\n",
    "        distribution[activity] = activities_counter[activity]/num_days\n",
    "\n",
    "    return distribution\n",
    "\n",
    "def print_simulation_distribution(days):\n",
    "    '''\n",
    "    This method calls the simulate days method and then prints a user-friendly text output.\n",
    "    '''\n",
    "    distribution = simulate_days(days)\n",
    "    print(\"A sampling distribution for the grad student's routine for {} days is:\".format(days))\n",
    "    print(\"{} of these days were spent watching movies\".format(distribution['M']))\n",
    "    print(\"{} of these days were spent doing COMP 551\".format(distribution['C']))\n",
    "    print(\"{} of these days were spent playing\".format(distribution['P']))\n",
    "    print(\"{} of these days were spent studying\".format(distribution['S']))\n",
    "    print(\"_______________________________________\")\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    print_simulation_distribution(100)\n",
    "    print_simulation_distribution(1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * * \n",
    "### Part 2 - Model Selection\n",
    "\n",
    "#### Question 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error for the Training Data using a degree 20 polynomial fit is 6.474680065000765\n",
      "The Mean Squared Error for the Validation Data using a degree 20 polynomial fit is 1418.5143681043703\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "This program has been specifically tailored to answer question 2 in assignment 1. Its methods\n",
    "and classes are specifically tailored for this question, however, I have tried to make them\n",
    "as abstract as possible for other similar programming needs.\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self,inp,out):\n",
    "        self.x = inp\n",
    "        self.y = out\n",
    "\n",
    "class PolynomialMaster():\n",
    "    '''\n",
    "    This class can be used to generate the best polynomial fits for a line\n",
    "    '''\n",
    "\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def get_best_fit(self,degree):\n",
    "        '''\n",
    "        Using the initialized data set this method finds the best fit polynomial\n",
    "        of the specified n-th degree. It uses the least squares regression method\n",
    "        and returns an array containing all the coefficients found with this method.\n",
    "        '''\n",
    "        rows = len(self.dataset.x)\n",
    "        cols = degree + 1\n",
    "        basis_array = np.zeros((rows, cols))\n",
    "\n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                basis_array[i][j] = PolynomialMaster.polynomial_basis_function(j, self.dataset.x[i])\n",
    "\n",
    "        mpps_of_basis = np.linalg.pinv(basis_array)\n",
    "\n",
    "        return np.matmul(mpps_of_basis,np.array(self.dataset.y))\n",
    "\n",
    "    def get_best_fit_l2(self,degree,lambda_val):\n",
    "        rows = len(self.dataset.x)\n",
    "        cols = degree + 1\n",
    "        basis_array = np.zeros((rows, cols))\n",
    "\n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                basis_array[i][j] = PolynomialMaster.polynomial_basis_function(j, self.dataset.x[i])\n",
    "\n",
    "        # output = np.matmul(basis_array.transpose(),basis_array)\n",
    "        # output = output + (lambda_val * np.identity(cols))\n",
    "        # output = np.linalg.inv(output)\n",
    "\n",
    "        output = np.linalg.inv((np.matmul(basis_array.transpose(),basis_array)+(lambda_val * np.identity(cols))))\n",
    "        output = np.matmul(np.matmul(output,basis_array.transpose()),self.dataset.y)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mean_squared_error(target_values, predictions):\n",
    "        output = 0\n",
    "        num_predictions = len(predictions)\n",
    "        for i in range(0, num_predictions):\n",
    "            output += ((target_values[i] - predictions[i]) ** 2)\n",
    "\n",
    "        return (output / num_predictions)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_polynomial_output(x,coefficients):\n",
    "        '''\n",
    "        Returns the output of the polynomial for a given x and its coefficients.\n",
    "        The coefficients must be in ascending order\n",
    "        '''\n",
    "        degree = len(coefficients)\n",
    "\n",
    "        output = 0\n",
    "        for power in range(0, degree):\n",
    "            output += coefficients[power] * (x ** power)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def polynomial_basis_function(power, x):\n",
    "        if power == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return x**power\n",
    "\n",
    "    @staticmethod\n",
    "    def get_predictions(inputs,coefficients):\n",
    "        predictions = []\n",
    "        for input in inputs:\n",
    "            predictions.append(PolynomialMaster.get_polynomial_output(input,coefficients))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "class Plotter():\n",
    "    def add_plot(self,x,y,plot_type='r.',label=None):\n",
    "        if label:\n",
    "            return plt.plot(x,y,plot_type,label)\n",
    "        else:\n",
    "            return plt.plot(x,y,plot_type)\n",
    "\n",
    "    def add_best_fit_poly(self,coefficients,plot_type='b-',num_sample_points=100,range=(-1,1)):\n",
    "        best_fit_x = np.linspace(range[0], range[1], num_sample_points)\n",
    "        best_fit_y = PolynomialMaster.get_polynomial_output(best_fit_x, coefficients)\n",
    "        plt.plot(best_fit_x,best_fit_y,plot_type)\n",
    "\n",
    "    def set_axis(self,axis):\n",
    "        plt.axis(axis)\n",
    "\n",
    "    def set_xLabel(self,label):\n",
    "        plt.xlabel(label)\n",
    "\n",
    "    def set_yLabel(self,label):\n",
    "        plt.ylabel(label)\n",
    "\n",
    "    def set_main_title(self,title):\n",
    "        plt.suptitle(title)\n",
    "\n",
    "    def show(self):\n",
    "        plt.show()\n",
    "\n",
    "    def modify_legend(self,**kwargs):\n",
    "        plt.legend(**kwargs)\n",
    "\n",
    "def initialize_data():\n",
    "    '''\n",
    "    This method calls the read_data method and initializes the data into the empty variables\n",
    "    created at the beginning of the program\n",
    "    '''\n",
    "    global TRAINING_DATA\n",
    "    TRAINING_DATA = read_data(\"Datasets/Dataset_1_train.csv\")\n",
    "    global VALIDATION_DATA\n",
    "    VALIDATION_DATA = read_data(\"Datasets/Dataset_1_valid.csv\")\n",
    "    global TEST_DATA\n",
    "    TEST_DATA = read_data(\"Datasets/Dataset_1_test.csv\")\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    inp = []\n",
    "    out = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            temp_line = line.split(',')\n",
    "            inp.append(float(temp_line[0]))\n",
    "            out.append(float(temp_line[1]))\n",
    "    return DataSet(inp, out)\n",
    "\n",
    "def divide_interval(interval,num_divisions,end_point_inclusive=True):\n",
    "    '''\n",
    "    This method takes in a tuple that represents an interval and returns a list with the given number\n",
    "    of divisions. If the variable end_point_inclusive is set to true then the returned list has\n",
    "    the end_point_included\n",
    "    '''\n",
    "    output = [interval[0]]\n",
    "    if interval[0] < interval[1]:\n",
    "        delta = (interval[1] - interval[0])/num_divisions\n",
    "        point = interval[0] + delta\n",
    "        while point < interval[1]:\n",
    "            output.append(point)\n",
    "            point+=delta\n",
    "\n",
    "    if end_point_inclusive:\n",
    "        output.append(interval[1])\n",
    "\n",
    "    return output\n",
    "\n",
    "def find_min(dict_in):\n",
    "    '''\n",
    "    Takes a dictionary as input and returns the key and val with the smallest value\n",
    "    '''\n",
    "    if len(dict_in) < 1:\n",
    "        return None\n",
    "\n",
    "    min_key = list(dict_in.keys())[0]\n",
    "    min_value = dict_in[min_key]\n",
    "    output = [min_key,min_value]\n",
    "\n",
    "    for key,val in dict_in.items():\n",
    "        if val < output[1]:\n",
    "            output[0] = key\n",
    "            output[1] = val\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def run_part_1():\n",
    "    #Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "\n",
    "    #Load training data into class and generate coefficients for best fit polynomial\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "    coefficients = poly_master.get_best_fit(degree)\n",
    "\n",
    "    #Get Mean Squared Error with validation data\n",
    "    training_predictions = poly_master.get_predictions(TRAINING_DATA.x, coefficients)\n",
    "    validation_predictions = poly_master.get_predictions(VALIDATION_DATA.x, coefficients)\n",
    "    training_mse = poly_master.get_mean_squared_error(TRAINING_DATA.y,training_predictions)\n",
    "    validation_mse = poly_master.get_mean_squared_error(VALIDATION_DATA.y,validation_predictions)\n",
    "    print(\"The Mean Squared Error for the Training Data using a degree {} polynomial fit is {}\".format(degree,training_mse))\n",
    "    print(\"The Mean Squared Error for the Validation Data using a degree {} polynomial fit is {}\".format(degree,validation_mse))\n",
    "\n",
    "    plotter.add_plot(TRAINING_DATA.x,TRAINING_DATA.y,'ro')\n",
    "    plotter.add_best_fit_poly(coefficients)\n",
    "    plotter.set_axis([-1,1,-40,40])\n",
    "    plotter.set_xLabel(\"Observations\")\n",
    "    plotter.set_yLabel(\"Results\")\n",
    "    plotter.set_main_title(\"Polynomial Curve Fitting with a {} Degree Polynomial\".format(degree))\n",
    "    plotter.show()\n",
    "\n",
    "\n",
    "def run_part_2_a():\n",
    "    # Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "    lambda_divisions = 100\n",
    "    mse_training = {}\n",
    "    mse_validation = {}\n",
    "\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "\n",
    "    lambda_values = divide_interval([0,1],lambda_divisions,True)\n",
    "    for val in lambda_values:\n",
    "        coefficients = poly_master.get_best_fit_l2(degree,val)\n",
    "        training_predictions = poly_master.get_predictions(TRAINING_DATA.x,coefficients)\n",
    "        validation_predictions = poly_master.get_predictions(VALIDATION_DATA.x,coefficients)\n",
    "\n",
    "        mse_training[val] = poly_master.get_mean_squared_error(TRAINING_DATA.y,training_predictions)\n",
    "        mse_validation[val] = poly_master.get_mean_squared_error(VALIDATION_DATA.y,validation_predictions)\n",
    "\n",
    "\n",
    "    plot1, = plt.plot(mse_training.keys(),mse_training.values(),'b-',label=\"Training Data MSE\")\n",
    "    plot2, = plt.plot(mse_validation.keys(),mse_validation.values(),'r-',label=\"Validation Data MSE\")\n",
    "    plotter.set_axis([0,1,0,15])\n",
    "    plotter.set_xLabel(\"Lambda Values\")\n",
    "    plotter.set_yLabel(\"Mean Squared Error\")\n",
    "    plotter.set_main_title(\"Mean Squared Error Values for Different Lambda Values\")\n",
    "    plotter.modify_legend()\n",
    "    plotter.show()\n",
    "\n",
    "def run_part_2_b():\n",
    "    # Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "\n",
    "    lambda_divisions = 100\n",
    "    mse_training = {}\n",
    "    mse_validation = {}\n",
    "\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "\n",
    "    lambda_values = divide_interval([0, 1], lambda_divisions, True)\n",
    "    for val in lambda_values:\n",
    "        coefficients = poly_master.get_best_fit_l2(degree, val)\n",
    "        training_predictions = poly_master.get_predictions(TRAINING_DATA.x, coefficients)\n",
    "        validation_predictions = poly_master.get_predictions(VALIDATION_DATA.x, coefficients)\n",
    "\n",
    "        mse_training[val] = poly_master.get_mean_squared_error(TRAINING_DATA.y, training_predictions)\n",
    "        mse_validation[val] = poly_master.get_mean_squared_error(VALIDATION_DATA.y, validation_predictions)\n",
    "\n",
    "    smallest_mse= find_min(mse_validation)\n",
    "    print(\"The smallest mean squared error found for the validation \"\n",
    "          \"data is {} with lambda value = {}\".format(smallest_mse[1],smallest_mse[0]))\n",
    "\n",
    "def run_part_2_c(lambda_val):\n",
    "    # Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "\n",
    "    # Load training data into class and generate coefficients for best fit polynomial\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "    coefficients = poly_master.get_best_fit_l2(degree,lambda_val=lambda_val)\n",
    "\n",
    "    plotter.add_plot(TRAINING_DATA.x, TRAINING_DATA.y, 'ro')\n",
    "    plotter.add_best_fit_poly(coefficients)\n",
    "    plotter.set_axis([-1, 1, -40, 40])\n",
    "    plotter.set_xLabel(\"Observations\")\n",
    "    plotter.set_yLabel(\"Results\")\n",
    "    plotter.set_main_title(\"Polynomial Curve Fitting with a {} Degree Polynomial\".format(degree))\n",
    "    plotter.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_part_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 \n",
    "##### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "This program has been specifically tailored to answer question 2 in assignment 1. Its methods\n",
    "and classes are specifically tailored for this question, however, I have tried to make them\n",
    "as abstract as possible for other similar programming needs.\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self,inp,out):\n",
    "        self.x = inp\n",
    "        self.y = out\n",
    "\n",
    "class PolynomialMaster():\n",
    "    '''\n",
    "    This class can be used to generate the best polynomial fits for a line\n",
    "    '''\n",
    "\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def get_best_fit(self,degree):\n",
    "        '''\n",
    "        Using the initialized data set this method finds the best fit polynomial\n",
    "        of the specified n-th degree. It uses the least squares regression method\n",
    "        and returns an array containing all the coefficients found with this method.\n",
    "        '''\n",
    "        rows = len(self.dataset.x)\n",
    "        cols = degree + 1\n",
    "        basis_array = np.zeros((rows, cols))\n",
    "\n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                basis_array[i][j] = PolynomialMaster.polynomial_basis_function(j, self.dataset.x[i])\n",
    "\n",
    "        mpps_of_basis = np.linalg.pinv(basis_array)\n",
    "\n",
    "        return np.matmul(mpps_of_basis,np.array(self.dataset.y))\n",
    "\n",
    "    def get_best_fit_l2(self,degree,lambda_val):\n",
    "        rows = len(self.dataset.x)\n",
    "        cols = degree + 1\n",
    "        basis_array = np.zeros((rows, cols))\n",
    "\n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                basis_array[i][j] = PolynomialMaster.polynomial_basis_function(j, self.dataset.x[i])\n",
    "\n",
    "        # output = np.matmul(basis_array.transpose(),basis_array)\n",
    "        # output = output + (lambda_val * np.identity(cols))\n",
    "        # output = np.linalg.inv(output)\n",
    "\n",
    "        output = np.linalg.inv((np.matmul(basis_array.transpose(),basis_array)+(lambda_val * np.identity(cols))))\n",
    "        output = np.matmul(np.matmul(output,basis_array.transpose()),self.dataset.y)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mean_squared_error(target_values, predictions):\n",
    "        output = 0\n",
    "        num_predictions = len(predictions)\n",
    "        for i in range(0, num_predictions):\n",
    "            output += ((target_values[i] - predictions[i]) ** 2)\n",
    "\n",
    "        return (output / num_predictions)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_polynomial_output(x,coefficients):\n",
    "        '''\n",
    "        Returns the output of the polynomial for a given x and its coefficients.\n",
    "        The coefficients must be in ascending order\n",
    "        '''\n",
    "        degree = len(coefficients)\n",
    "\n",
    "        output = 0\n",
    "        for power in range(0, degree):\n",
    "            output += coefficients[power] * (x ** power)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def polynomial_basis_function(power, x):\n",
    "        if power == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return x**power\n",
    "\n",
    "    @staticmethod\n",
    "    def get_predictions(inputs,coefficients):\n",
    "        predictions = []\n",
    "        for input in inputs:\n",
    "            predictions.append(PolynomialMaster.get_polynomial_output(input,coefficients))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "class Plotter():\n",
    "    def add_plot(self,x,y,plot_type='r.',label=None):\n",
    "        if label:\n",
    "            return plt.plot(x,y,plot_type,label)\n",
    "        else:\n",
    "            return plt.plot(x,y,plot_type)\n",
    "\n",
    "    def add_best_fit_poly(self,coefficients,plot_type='b-',num_sample_points=100,range=(-1,1)):\n",
    "        best_fit_x = np.linspace(range[0], range[1], num_sample_points)\n",
    "        best_fit_y = PolynomialMaster.get_polynomial_output(best_fit_x, coefficients)\n",
    "        plt.plot(best_fit_x,best_fit_y,plot_type)\n",
    "\n",
    "    def set_axis(self,axis):\n",
    "        plt.axis(axis)\n",
    "\n",
    "    def set_xLabel(self,label):\n",
    "        plt.xlabel(label)\n",
    "\n",
    "    def set_yLabel(self,label):\n",
    "        plt.ylabel(label)\n",
    "\n",
    "    def set_main_title(self,title):\n",
    "        plt.suptitle(title)\n",
    "\n",
    "    def show(self):\n",
    "        plt.show()\n",
    "\n",
    "    def modify_legend(self,**kwargs):\n",
    "        plt.legend(**kwargs)\n",
    "\n",
    "def initialize_data():\n",
    "    '''\n",
    "    This method calls the read_data method and initializes the data into the empty variables\n",
    "    created at the beginning of the program\n",
    "    '''\n",
    "    global TRAINING_DATA\n",
    "    TRAINING_DATA = read_data(\"Datasets/Dataset_1_train.csv\")\n",
    "    global VALIDATION_DATA\n",
    "    VALIDATION_DATA = read_data(\"Datasets/Dataset_1_valid.csv\")\n",
    "    global TEST_DATA\n",
    "    TEST_DATA = read_data(\"Datasets/Dataset_1_test.csv\")\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    inp = []\n",
    "    out = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            temp_line = line.split(',')\n",
    "            inp.append(float(temp_line[0]))\n",
    "            out.append(float(temp_line[1]))\n",
    "    return DataSet(inp, out)\n",
    "\n",
    "def divide_interval(interval,num_divisions,end_point_inclusive=True):\n",
    "    '''\n",
    "    This method takes in a tuple that represents an interval and returns a list with the given number\n",
    "    of divisions. If the variable end_point_inclusive is set to true then the returned list has\n",
    "    the end_point_included\n",
    "    '''\n",
    "    output = [interval[0]]\n",
    "    if interval[0] < interval[1]:\n",
    "        delta = (interval[1] - interval[0])/num_divisions\n",
    "        point = interval[0] + delta\n",
    "        while point < interval[1]:\n",
    "            output.append(point)\n",
    "            point+=delta\n",
    "\n",
    "    if end_point_inclusive:\n",
    "        output.append(interval[1])\n",
    "\n",
    "    return output\n",
    "\n",
    "def find_min(dict_in):\n",
    "    '''\n",
    "    Takes a dictionary as input and returns the key and val with the smallest value\n",
    "    '''\n",
    "    if len(dict_in) < 1:\n",
    "        return None\n",
    "\n",
    "    min_key = list(dict_in.keys())[0]\n",
    "    min_value = dict_in[min_key]\n",
    "    output = [min_key,min_value]\n",
    "\n",
    "    for key,val in dict_in.items():\n",
    "        if val < output[1]:\n",
    "            output[0] = key\n",
    "            output[1] = val\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def run_part_1():\n",
    "    #Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "\n",
    "    #Load training data into class and generate coefficients for best fit polynomial\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "    coefficients = poly_master.get_best_fit(degree)\n",
    "\n",
    "    #Get Mean Squared Error with validation data\n",
    "    training_predictions = poly_master.get_predictions(TRAINING_DATA.x, coefficients)\n",
    "    validation_predictions = poly_master.get_predictions(VALIDATION_DATA.x, coefficients)\n",
    "    training_mse = poly_master.get_mean_squared_error(TRAINING_DATA.y,training_predictions)\n",
    "    validation_mse = poly_master.get_mean_squared_error(VALIDATION_DATA.y,validation_predictions)\n",
    "    print(\"The Mean Squared Error for the Training Data using a degree {} polynomial fit is {}\".format(degree,training_mse))\n",
    "    print(\"The Mean Squared Error for the Validation Data using a degree {} polynomial fit is {}\".format(degree,validation_mse))\n",
    "\n",
    "    plotter.add_plot(TRAINING_DATA.x,TRAINING_DATA.y,'ro')\n",
    "    plotter.add_best_fit_poly(coefficients)\n",
    "    plotter.set_axis([-1,1,-40,40])\n",
    "    plotter.set_xLabel(\"Observations\")\n",
    "    plotter.set_yLabel(\"Results\")\n",
    "    plotter.set_main_title(\"Polynomial Curve Fitting with a {} Degree Polynomial\".format(degree))\n",
    "    plotter.show()\n",
    "\n",
    "\n",
    "def run_part_2_a():\n",
    "    # Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "    lambda_divisions = 100\n",
    "    mse_training = {}\n",
    "    mse_validation = {}\n",
    "\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "\n",
    "    lambda_values = divide_interval([0,1],lambda_divisions,True)\n",
    "    for val in lambda_values:\n",
    "        coefficients = poly_master.get_best_fit_l2(degree,val)\n",
    "        training_predictions = poly_master.get_predictions(TRAINING_DATA.x,coefficients)\n",
    "        validation_predictions = poly_master.get_predictions(VALIDATION_DATA.x,coefficients)\n",
    "\n",
    "        mse_training[val] = poly_master.get_mean_squared_error(TRAINING_DATA.y,training_predictions)\n",
    "        mse_validation[val] = poly_master.get_mean_squared_error(VALIDATION_DATA.y,validation_predictions)\n",
    "\n",
    "\n",
    "    plot1, = plt.plot(mse_training.keys(),mse_training.values(),'b-',label=\"Training Data MSE\")\n",
    "    plot2, = plt.plot(mse_validation.keys(),mse_validation.values(),'r-',label=\"Validation Data MSE\")\n",
    "    plotter.set_axis([0,1,0,15])\n",
    "    plotter.set_xLabel(\"Lambda Values\")\n",
    "    plotter.set_yLabel(\"Mean Squared Error\")\n",
    "    plotter.set_main_title(\"Mean Squared Error Values for Different Lambda Values\")\n",
    "    plotter.modify_legend()\n",
    "    plotter.show()\n",
    "\n",
    "def run_part_2_b():\n",
    "    # Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "\n",
    "    lambda_divisions = 100\n",
    "    mse_training = {}\n",
    "    mse_validation = {}\n",
    "\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "\n",
    "    lambda_values = divide_interval([0, 1], lambda_divisions, True)\n",
    "    for val in lambda_values:\n",
    "        coefficients = poly_master.get_best_fit_l2(degree, val)\n",
    "        training_predictions = poly_master.get_predictions(TRAINING_DATA.x, coefficients)\n",
    "        validation_predictions = poly_master.get_predictions(VALIDATION_DATA.x, coefficients)\n",
    "\n",
    "        mse_training[val] = poly_master.get_mean_squared_error(TRAINING_DATA.y, training_predictions)\n",
    "        mse_validation[val] = poly_master.get_mean_squared_error(VALIDATION_DATA.y, validation_predictions)\n",
    "\n",
    "    smallest_mse= find_min(mse_validation)\n",
    "    print(\"The smallest mean squared error found for the validation \"\n",
    "          \"data is {} with lambda value = {}\".format(smallest_mse[1],smallest_mse[0]))\n",
    "\n",
    "def run_part_2_c(lambda_val):\n",
    "    # Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "\n",
    "    # Load training data into class and generate coefficients for best fit polynomial\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "    coefficients = poly_master.get_best_fit_l2(degree,lambda_val=lambda_val)\n",
    "\n",
    "    plotter.add_plot(TRAINING_DATA.x, TRAINING_DATA.y, 'ro')\n",
    "    plotter.add_best_fit_poly(coefficients)\n",
    "    plotter.set_axis([-1, 1, -40, 40])\n",
    "    plotter.set_xLabel(\"Observations\")\n",
    "    plotter.set_yLabel(\"Results\")\n",
    "    plotter.set_main_title(\"Polynomial Curve Fitting with a {} Degree Polynomial\".format(degree))\n",
    "    plotter.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_part_2_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smallest mean squared error found for the validation data is 9.135098784694396 with lambda value = 0.02\n",
      "Let's now test this with test data:\n",
      "The M.S.E. using lambda = 0.02 on the test data is 10.730218400927388\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This program has been specifically tailored to answer question 2 in assignment 1. Its methods\n",
    "and classes are specifically tailored for this question, however, I have tried to make them\n",
    "as abstract as possible for other similar programming needs.\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self,inp,out):\n",
    "        self.x = inp\n",
    "        self.y = out\n",
    "\n",
    "class PolynomialMaster():\n",
    "    '''\n",
    "    This class can be used to generate the best polynomial fits for a line\n",
    "    '''\n",
    "\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def get_best_fit(self,degree):\n",
    "        '''\n",
    "        Using the initialized data set this method finds the best fit polynomial\n",
    "        of the specified n-th degree. It uses the least squares regression method\n",
    "        and returns an array containing all the coefficients found with this method.\n",
    "        '''\n",
    "        rows = len(self.dataset.x)\n",
    "        cols = degree + 1\n",
    "        basis_array = np.zeros((rows, cols))\n",
    "\n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                basis_array[i][j] = PolynomialMaster.polynomial_basis_function(j, self.dataset.x[i])\n",
    "\n",
    "        mpps_of_basis = np.linalg.pinv(basis_array)\n",
    "\n",
    "        return np.matmul(mpps_of_basis,np.array(self.dataset.y))\n",
    "\n",
    "    def get_best_fit_l2(self,degree,lambda_val):\n",
    "        rows = len(self.dataset.x)\n",
    "        cols = degree + 1\n",
    "        basis_array = np.zeros((rows, cols))\n",
    "\n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                basis_array[i][j] = PolynomialMaster.polynomial_basis_function(j, self.dataset.x[i])\n",
    "\n",
    "        # output = np.matmul(basis_array.transpose(),basis_array)\n",
    "        # output = output + (lambda_val * np.identity(cols))\n",
    "        # output = np.linalg.inv(output)\n",
    "\n",
    "        output = np.linalg.inv((np.matmul(basis_array.transpose(),basis_array)+(lambda_val * np.identity(cols))))\n",
    "        output = np.matmul(np.matmul(output,basis_array.transpose()),self.dataset.y)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mean_squared_error(target_values, predictions):\n",
    "        output = 0\n",
    "        num_predictions = len(predictions)\n",
    "        for i in range(0, num_predictions):\n",
    "            output += ((target_values[i] - predictions[i]) ** 2)\n",
    "\n",
    "        return (output / num_predictions)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_polynomial_output(x,coefficients):\n",
    "        '''\n",
    "        Returns the output of the polynomial for a given x and its coefficients.\n",
    "        The coefficients must be in ascending order\n",
    "        '''\n",
    "        degree = len(coefficients)\n",
    "\n",
    "        output = 0\n",
    "        for power in range(0, degree):\n",
    "            output += coefficients[power] * (x ** power)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def polynomial_basis_function(power, x):\n",
    "        if power == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return x**power\n",
    "\n",
    "    @staticmethod\n",
    "    def get_predictions(inputs,coefficients):\n",
    "        predictions = []\n",
    "        for input in inputs:\n",
    "            predictions.append(PolynomialMaster.get_polynomial_output(input,coefficients))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "class Plotter():\n",
    "    def add_plot(self,x,y,plot_type='r.',label=None):\n",
    "        if label:\n",
    "            return plt.plot(x,y,plot_type,label)\n",
    "        else:\n",
    "            return plt.plot(x,y,plot_type)\n",
    "\n",
    "    def add_best_fit_poly(self,coefficients,plot_type='b-',num_sample_points=100,range=(-1,1)):\n",
    "        best_fit_x = np.linspace(range[0], range[1], num_sample_points)\n",
    "        best_fit_y = PolynomialMaster.get_polynomial_output(best_fit_x, coefficients)\n",
    "        plt.plot(best_fit_x,best_fit_y,plot_type)\n",
    "\n",
    "    def set_axis(self,axis):\n",
    "        plt.axis(axis)\n",
    "\n",
    "    def set_xLabel(self,label):\n",
    "        plt.xlabel(label)\n",
    "\n",
    "    def set_yLabel(self,label):\n",
    "        plt.ylabel(label)\n",
    "\n",
    "    def set_main_title(self,title):\n",
    "        plt.suptitle(title)\n",
    "\n",
    "    def show(self):\n",
    "        plt.show()\n",
    "\n",
    "    def modify_legend(self,**kwargs):\n",
    "        plt.legend(**kwargs)\n",
    "\n",
    "def initialize_data():\n",
    "    '''\n",
    "    This method calls the read_data method and initializes the data into the empty variables\n",
    "    created at the beginning of the program\n",
    "    '''\n",
    "    global TRAINING_DATA\n",
    "    TRAINING_DATA = read_data(\"Datasets/Dataset_1_train.csv\")\n",
    "    global VALIDATION_DATA\n",
    "    VALIDATION_DATA = read_data(\"Datasets/Dataset_1_valid.csv\")\n",
    "    global TEST_DATA\n",
    "    TEST_DATA = read_data(\"Datasets/Dataset_1_test.csv\")\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    inp = []\n",
    "    out = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            temp_line = line.split(',')\n",
    "            inp.append(float(temp_line[0]))\n",
    "            out.append(float(temp_line[1]))\n",
    "    return DataSet(inp, out)\n",
    "\n",
    "def divide_interval(interval,num_divisions,end_point_inclusive=True):\n",
    "    '''\n",
    "    This method takes in a tuple that represents an interval and returns a list with the given number\n",
    "    of divisions. If the variable end_point_inclusive is set to true then the returned list has\n",
    "    the end_point_included\n",
    "    '''\n",
    "    output = [interval[0]]\n",
    "    if interval[0] < interval[1]:\n",
    "        delta = (interval[1] - interval[0])/num_divisions\n",
    "        point = interval[0] + delta\n",
    "        while point < interval[1]:\n",
    "            output.append(point)\n",
    "            point+=delta\n",
    "\n",
    "    if end_point_inclusive:\n",
    "        output.append(interval[1])\n",
    "\n",
    "    return output\n",
    "\n",
    "def find_min(dict_in):\n",
    "    '''\n",
    "    Takes a dictionary as input and returns the key and val with the smallest value\n",
    "    '''\n",
    "    if len(dict_in) < 1:\n",
    "        return None\n",
    "\n",
    "    min_key = list(dict_in.keys())[0]\n",
    "    min_value = dict_in[min_key]\n",
    "    output = [min_key,min_value]\n",
    "\n",
    "    for key,val in dict_in.items():\n",
    "        if val < output[1]:\n",
    "            output[0] = key\n",
    "            output[1] = val\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def run_part_1():\n",
    "    #Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "\n",
    "    #Load training data into class and generate coefficients for best fit polynomial\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "    coefficients = poly_master.get_best_fit(degree)\n",
    "\n",
    "    #Get Mean Squared Error with validation data\n",
    "    training_predictions = poly_master.get_predictions(TRAINING_DATA.x, coefficients)\n",
    "    validation_predictions = poly_master.get_predictions(VALIDATION_DATA.x, coefficients)\n",
    "    training_mse = poly_master.get_mean_squared_error(TRAINING_DATA.y,training_predictions)\n",
    "    validation_mse = poly_master.get_mean_squared_error(VALIDATION_DATA.y,validation_predictions)\n",
    "    print(\"The Mean Squared Error for the Training Data using a degree {} polynomial fit is {}\".format(degree,training_mse))\n",
    "    print(\"The Mean Squared Error for the Validation Data using a degree {} polynomial fit is {}\".format(degree,validation_mse))\n",
    "\n",
    "    plotter.add_plot(TRAINING_DATA.x,TRAINING_DATA.y,'ro')\n",
    "    plotter.add_best_fit_poly(coefficients)\n",
    "    plotter.set_axis([-1,1,-40,40])\n",
    "    plotter.set_xLabel(\"Observations\")\n",
    "    plotter.set_yLabel(\"Results\")\n",
    "    plotter.set_main_title(\"Polynomial Curve Fitting with a {} Degree Polynomial\".format(degree))\n",
    "    plotter.show()\n",
    "\n",
    "\n",
    "def run_part_2_a():\n",
    "    # Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "    lambda_divisions = 100\n",
    "    mse_training = {}\n",
    "    mse_validation = {}\n",
    "\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "\n",
    "    lambda_values = divide_interval([0,1],lambda_divisions,True)\n",
    "    for val in lambda_values:\n",
    "        coefficients = poly_master.get_best_fit_l2(degree,val)\n",
    "        training_predictions = poly_master.get_predictions(TRAINING_DATA.x,coefficients)\n",
    "        validation_predictions = poly_master.get_predictions(VALIDATION_DATA.x,coefficients)\n",
    "\n",
    "        mse_training[val] = poly_master.get_mean_squared_error(TRAINING_DATA.y,training_predictions)\n",
    "        mse_validation[val] = poly_master.get_mean_squared_error(VALIDATION_DATA.y,validation_predictions)\n",
    "\n",
    "\n",
    "    plot1, = plt.plot(mse_training.keys(),mse_training.values(),'b-',label=\"Training Data MSE\")\n",
    "    plot2, = plt.plot(mse_validation.keys(),mse_validation.values(),'r-',label=\"Validation Data MSE\")\n",
    "    plotter.set_axis([0,1,0,15])\n",
    "    plotter.set_xLabel(\"Lambda Values\")\n",
    "    plotter.set_yLabel(\"Mean Squared Error\")\n",
    "    plotter.set_main_title(\"Mean Squared Error Values for Different Lambda Values\")\n",
    "    plotter.modify_legend()\n",
    "    plotter.show()\n",
    "\n",
    "def run_part_2_b():\n",
    "    # Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "\n",
    "    lambda_divisions = 100\n",
    "    mse_training = {}\n",
    "    mse_validation = {}\n",
    "\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "\n",
    "    lambda_values = divide_interval([0, 1], lambda_divisions, True)\n",
    "    for val in lambda_values:\n",
    "        coefficients = poly_master.get_best_fit_l2(degree, val)\n",
    "        training_predictions = poly_master.get_predictions(TRAINING_DATA.x, coefficients)\n",
    "        validation_predictions = poly_master.get_predictions(VALIDATION_DATA.x, coefficients)\n",
    "\n",
    "        mse_training[val] = poly_master.get_mean_squared_error(TRAINING_DATA.y, training_predictions)\n",
    "        mse_validation[val] = poly_master.get_mean_squared_error(VALIDATION_DATA.y, validation_predictions)\n",
    "\n",
    "    smallest_mse= find_min(mse_validation)\n",
    "    print(\"The smallest mean squared error found for the validation \"\n",
    "          \"data is {} with lambda value = {}\".format(smallest_mse[1],smallest_mse[0]))\n",
    "\n",
    "    print(\"Let's now test this with test data:\")\n",
    "\n",
    "    coefficients = poly_master.get_best_fit_l2(degree,smallest_mse[0])\n",
    "    test_predictions = poly_master.get_predictions(TEST_DATA.x,coefficients)\n",
    "    test_mse = poly_master.get_mean_squared_error(TEST_DATA.y,test_predictions)\n",
    "\n",
    "    print(\"The M.S.E. using lambda = {} on the test data is {}\".format(smallest_mse[0],test_mse))\n",
    "\n",
    "def run_part_2_c(lambda_val):\n",
    "    # Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "\n",
    "    # Load training data into class and generate coefficients for best fit polynomial\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "    coefficients = poly_master.get_best_fit_l2(degree,lambda_val=lambda_val)\n",
    "\n",
    "    plotter.add_plot(TRAINING_DATA.x, TRAINING_DATA.y, 'ro')\n",
    "    plotter.add_best_fit_poly(coefficients)\n",
    "    plotter.set_axis([-1, 1, -40, 40])\n",
    "    plotter.set_xLabel(\"Observations\")\n",
    "    plotter.set_yLabel(\"Results\")\n",
    "    plotter.set_main_title(\"Polynomial Curve Fitting with a {} Degree Polynomial\".format(degree))\n",
    "    plotter.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_part_2_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "This program has been specifically tailored to answer question 2 in assignment 1. Its methods\n",
    "and classes are specifically tailored for this question, however, I have tried to make them\n",
    "as abstract as possible for other similar programming needs.\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self,inp,out):\n",
    "        self.x = inp\n",
    "        self.y = out\n",
    "\n",
    "class PolynomialMaster():\n",
    "    '''\n",
    "    This class can be used to generate the best polynomial fits for a line\n",
    "    '''\n",
    "\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def get_best_fit(self,degree):\n",
    "        '''\n",
    "        Using the initialized data set this method finds the best fit polynomial\n",
    "        of the specified n-th degree. It uses the least squares regression method\n",
    "        and returns an array containing all the coefficients found with this method.\n",
    "        '''\n",
    "        rows = len(self.dataset.x)\n",
    "        cols = degree + 1\n",
    "        basis_array = np.zeros((rows, cols))\n",
    "\n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                basis_array[i][j] = PolynomialMaster.polynomial_basis_function(j, self.dataset.x[i])\n",
    "\n",
    "        mpps_of_basis = np.linalg.pinv(basis_array)\n",
    "\n",
    "        return np.matmul(mpps_of_basis,np.array(self.dataset.y))\n",
    "\n",
    "    def get_best_fit_l2(self,degree,lambda_val):\n",
    "        rows = len(self.dataset.x)\n",
    "        cols = degree + 1\n",
    "        basis_array = np.zeros((rows, cols))\n",
    "\n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                basis_array[i][j] = PolynomialMaster.polynomial_basis_function(j, self.dataset.x[i])\n",
    "\n",
    "        # output = np.matmul(basis_array.transpose(),basis_array)\n",
    "        # output = output + (lambda_val * np.identity(cols))\n",
    "        # output = np.linalg.inv(output)\n",
    "\n",
    "        output = np.linalg.inv((np.matmul(basis_array.transpose(),basis_array)+(lambda_val * np.identity(cols))))\n",
    "        output = np.matmul(np.matmul(output,basis_array.transpose()),self.dataset.y)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mean_squared_error(target_values, predictions):\n",
    "        output = 0\n",
    "        num_predictions = len(predictions)\n",
    "        for i in range(0, num_predictions):\n",
    "            output += ((target_values[i] - predictions[i]) ** 2)\n",
    "\n",
    "        return (output / num_predictions)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_polynomial_output(x,coefficients):\n",
    "        '''\n",
    "        Returns the output of the polynomial for a given x and its coefficients.\n",
    "        The coefficients must be in ascending order\n",
    "        '''\n",
    "        degree = len(coefficients)\n",
    "\n",
    "        output = 0\n",
    "        for power in range(0, degree):\n",
    "            output += coefficients[power] * (x ** power)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def polynomial_basis_function(power, x):\n",
    "        if power == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return x**power\n",
    "\n",
    "    @staticmethod\n",
    "    def get_predictions(inputs,coefficients):\n",
    "        predictions = []\n",
    "        for input in inputs:\n",
    "            predictions.append(PolynomialMaster.get_polynomial_output(input,coefficients))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "class Plotter():\n",
    "    def add_plot(self,x,y,plot_type='r.',label=None):\n",
    "        if label:\n",
    "            return plt.plot(x,y,plot_type,label)\n",
    "        else:\n",
    "            return plt.plot(x,y,plot_type)\n",
    "\n",
    "    def add_best_fit_poly(self,coefficients,plot_type='b-',num_sample_points=100,range=(-1,1)):\n",
    "        best_fit_x = np.linspace(range[0], range[1], num_sample_points)\n",
    "        best_fit_y = PolynomialMaster.get_polynomial_output(best_fit_x, coefficients)\n",
    "        plt.plot(best_fit_x,best_fit_y,plot_type)\n",
    "\n",
    "    def set_axis(self,axis):\n",
    "        plt.axis(axis)\n",
    "\n",
    "    def set_xLabel(self,label):\n",
    "        plt.xlabel(label)\n",
    "\n",
    "    def set_yLabel(self,label):\n",
    "        plt.ylabel(label)\n",
    "\n",
    "    def set_main_title(self,title):\n",
    "        plt.suptitle(title)\n",
    "\n",
    "    def show(self):\n",
    "        plt.show()\n",
    "\n",
    "    def modify_legend(self,**kwargs):\n",
    "        plt.legend(**kwargs)\n",
    "\n",
    "def initialize_data():\n",
    "    '''\n",
    "    This method calls the read_data method and initializes the data into the empty variables\n",
    "    created at the beginning of the program\n",
    "    '''\n",
    "    global TRAINING_DATA\n",
    "    TRAINING_DATA = read_data(\"Datasets/Dataset_1_train.csv\")\n",
    "    global VALIDATION_DATA\n",
    "    VALIDATION_DATA = read_data(\"Datasets/Dataset_1_valid.csv\")\n",
    "    global TEST_DATA\n",
    "    TEST_DATA = read_data(\"Datasets/Dataset_1_test.csv\")\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    inp = []\n",
    "    out = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            temp_line = line.split(',')\n",
    "            inp.append(float(temp_line[0]))\n",
    "            out.append(float(temp_line[1]))\n",
    "    return DataSet(inp, out)\n",
    "\n",
    "def divide_interval(interval,num_divisions,end_point_inclusive=True):\n",
    "    '''\n",
    "    This method takes in a tuple that represents an interval and returns a list with the given number\n",
    "    of divisions. If the variable end_point_inclusive is set to true then the returned list has\n",
    "    the end_point_included\n",
    "    '''\n",
    "    output = [interval[0]]\n",
    "    if interval[0] < interval[1]:\n",
    "        delta = (interval[1] - interval[0])/num_divisions\n",
    "        point = interval[0] + delta\n",
    "        while point < interval[1]:\n",
    "            output.append(point)\n",
    "            point+=delta\n",
    "\n",
    "    if end_point_inclusive:\n",
    "        output.append(interval[1])\n",
    "\n",
    "    return output\n",
    "\n",
    "def find_min(dict_in):\n",
    "    '''\n",
    "    Takes a dictionary as input and returns the key and val with the smallest value\n",
    "    '''\n",
    "    if len(dict_in) < 1:\n",
    "        return None\n",
    "\n",
    "    min_key = list(dict_in.keys())[0]\n",
    "    min_value = dict_in[min_key]\n",
    "    output = [min_key,min_value]\n",
    "\n",
    "    for key,val in dict_in.items():\n",
    "        if val < output[1]:\n",
    "            output[0] = key\n",
    "            output[1] = val\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def run_part_1():\n",
    "    #Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "\n",
    "    #Load training data into class and generate coefficients for best fit polynomial\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "    coefficients = poly_master.get_best_fit(degree)\n",
    "\n",
    "    #Get Mean Squared Error with validation data\n",
    "    training_predictions = poly_master.get_predictions(TRAINING_DATA.x, coefficients)\n",
    "    validation_predictions = poly_master.get_predictions(VALIDATION_DATA.x, coefficients)\n",
    "    training_mse = poly_master.get_mean_squared_error(TRAINING_DATA.y,training_predictions)\n",
    "    validation_mse = poly_master.get_mean_squared_error(VALIDATION_DATA.y,validation_predictions)\n",
    "    print(\"The Mean Squared Error for the Training Data using a degree {} polynomial fit is {}\".format(degree,training_mse))\n",
    "    print(\"The Mean Squared Error for the Validation Data using a degree {} polynomial fit is {}\".format(degree,validation_mse))\n",
    "\n",
    "    plotter.add_plot(TRAINING_DATA.x,TRAINING_DATA.y,'ro')\n",
    "    plotter.add_best_fit_poly(coefficients)\n",
    "    plotter.set_axis([-1,1,-40,40])\n",
    "    plotter.set_xLabel(\"Observations\")\n",
    "    plotter.set_yLabel(\"Results\")\n",
    "    plotter.set_main_title(\"Polynomial Curve Fitting with a {} Degree Polynomial\".format(degree))\n",
    "    plotter.show()\n",
    "\n",
    "\n",
    "def run_part_2_a():\n",
    "    # Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "    lambda_divisions = 100\n",
    "    mse_training = {}\n",
    "    mse_validation = {}\n",
    "\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "\n",
    "    lambda_values = divide_interval([0,1],lambda_divisions,True)\n",
    "    for val in lambda_values:\n",
    "        coefficients = poly_master.get_best_fit_l2(degree,val)\n",
    "        training_predictions = poly_master.get_predictions(TRAINING_DATA.x,coefficients)\n",
    "        validation_predictions = poly_master.get_predictions(VALIDATION_DATA.x,coefficients)\n",
    "\n",
    "        mse_training[val] = poly_master.get_mean_squared_error(TRAINING_DATA.y,training_predictions)\n",
    "        mse_validation[val] = poly_master.get_mean_squared_error(VALIDATION_DATA.y,validation_predictions)\n",
    "\n",
    "\n",
    "    plot1, = plt.plot(mse_training.keys(),mse_training.values(),'b-',label=\"Training Data MSE\")\n",
    "    plot2, = plt.plot(mse_validation.keys(),mse_validation.values(),'r-',label=\"Validation Data MSE\")\n",
    "    plotter.set_axis([0,1,0,15])\n",
    "    plotter.set_xLabel(\"Lambda Values\")\n",
    "    plotter.set_yLabel(\"Mean Squared Error\")\n",
    "    plotter.set_main_title(\"Mean Squared Error Values for Different Lambda Values\")\n",
    "    plotter.modify_legend()\n",
    "    plotter.show()\n",
    "\n",
    "def run_part_2_b():\n",
    "    # Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "\n",
    "    lambda_divisions = 100\n",
    "    mse_training = {}\n",
    "    mse_validation = {}\n",
    "\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "\n",
    "    lambda_values = divide_interval([0, 1], lambda_divisions, True)\n",
    "    for val in lambda_values:\n",
    "        coefficients = poly_master.get_best_fit_l2(degree, val)\n",
    "        training_predictions = poly_master.get_predictions(TRAINING_DATA.x, coefficients)\n",
    "        validation_predictions = poly_master.get_predictions(VALIDATION_DATA.x, coefficients)\n",
    "\n",
    "        mse_training[val] = poly_master.get_mean_squared_error(TRAINING_DATA.y, training_predictions)\n",
    "        mse_validation[val] = poly_master.get_mean_squared_error(VALIDATION_DATA.y, validation_predictions)\n",
    "\n",
    "    smallest_mse= find_min(mse_validation)\n",
    "    print(\"The smallest mean squared error found for the validation \"\n",
    "          \"data is {} with lambda value = {}\".format(smallest_mse[1],smallest_mse[0]))\n",
    "\n",
    "def run_part_2_c(lambda_val):\n",
    "    # Initialize data, classes, and variables\n",
    "    initialize_data()\n",
    "    plotter = Plotter()\n",
    "    degree = 20\n",
    "\n",
    "    # Load training data into class and generate coefficients for best fit polynomial\n",
    "    poly_master = PolynomialMaster(TRAINING_DATA)\n",
    "    coefficients = poly_master.get_best_fit_l2(degree,lambda_val=lambda_val)\n",
    "\n",
    "    plotter.add_plot(TRAINING_DATA.x, TRAINING_DATA.y, 'ro')\n",
    "    plotter.add_best_fit_poly(coefficients)\n",
    "    plotter.set_axis([-1, 1, -40, 40])\n",
    "    plotter.set_xLabel(\"Observations\")\n",
    "    plotter.set_yLabel(\"Results\")\n",
    "    plotter.set_main_title(\"Polynomial Curve Fitting with a {} Degree Polynomial\".format(degree))\n",
    "    plotter.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_part_2_c(0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "### Part 3 - Gradient Descent For Regression\n",
    "#### Question 1\n",
    "##### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been initialized: \n",
      "The initial parameters are set to (0.457665570248999,0.0897154297463858) \n",
      "The step size is set to 1e-06\n",
      "The number of epochs is set to 10000 \n",
      "The MSE for the last epoch of 10000 is 0.19608486965211275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self,inp,out):\n",
    "        self.x = inp\n",
    "        self.y = out\n",
    "\n",
    "\n",
    "def get_mean_squared_error(target_values, predictions):\n",
    "    output = 0\n",
    "    num_predictions = len(predictions)\n",
    "    for i in range(0, num_predictions):\n",
    "        output += ((target_values[i] - predictions[i]) ** 2)\n",
    "\n",
    "    return (output / num_predictions)\n",
    "\n",
    "\n",
    "def get_polynomial_output(x,coefficients):\n",
    "    '''\n",
    "    Returns the output of the polynomial for a given x and its coefficients.\n",
    "    The coefficients must be in ascending order\n",
    "    '''\n",
    "    degree = len(coefficients)\n",
    "\n",
    "    output = 0\n",
    "    for power in range(0, degree):\n",
    "        output += coefficients[power] * (x ** power)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_predictions(inputs,coefficients):\n",
    "    predictions = []\n",
    "    for input in inputs:\n",
    "        predictions.append(get_polynomial_output(input,coefficients))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(dataset,parameters,step_size,num_epochs):\n",
    "    param_data = []\n",
    "\n",
    "    num_points = len(dataset.x)\n",
    "    data_array = np.zeros((num_points,2))       #numpy 2D array to hold given dataset\n",
    "\n",
    "    #This is a N*2 matrix where N is the number of data points. Each row contains the x and y values in that order\n",
    "    for point in range(0,num_points):\n",
    "        data_array[point][0] = dataset.x[point]\n",
    "        data_array[point][1] = dataset.y[point]\n",
    "\n",
    "    for i in range(0,num_epochs):\n",
    "        temp_data_array = np.copy(data_array)\n",
    "        np.random.shuffle(temp_data_array)\n",
    "        for j in range(0,num_points):\n",
    "            x = temp_data_array[j][0]\n",
    "            y = temp_data_array[j][1]\n",
    "            prediction = get_predictions([x],parameters)[0]\n",
    "\n",
    "            parameters[0] = parameters[0] - step_size*(prediction - y)\n",
    "            parameters[1] = parameters[1] - step_size*(prediction - y)*x\n",
    "\n",
    "        param_data.append([parameters[0], parameters[1]])\n",
    "    return param_data\n",
    "\n",
    "\n",
    "def initialize_data():\n",
    "    '''\n",
    "    This method calls the read_data method and initializes the data into the empty variables\n",
    "    created at the beginning of the program\n",
    "    '''\n",
    "    global TRAINING_DATA\n",
    "    TRAINING_DATA = read_data(\"Datasets/Dataset_2_train.csv\")\n",
    "    global VALIDATION_DATA\n",
    "    VALIDATION_DATA = read_data(\"Datasets/Dataset_2_valid.csv\")\n",
    "    global TEST_DATA\n",
    "    TEST_DATA = read_data(\"Datasets/Dataset_2_test.csv\")\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    inp = []\n",
    "    out = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            temp_line = line.split(',')\n",
    "            inp.append(float(temp_line[0]))\n",
    "            out.append(float(temp_line[1]))\n",
    "    return DataSet(inp, out)\n",
    "\n",
    "def run_q1_a():\n",
    "    initialize_data()\n",
    "    initial_params = [random(),random()]\n",
    "    step_size = 10**(-6)\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0],initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                parameters=initial_params,\n",
    "                                step_size=step_size,\n",
    "                                num_epochs=num_epochs)\n",
    "\n",
    "    epochs = []\n",
    "    mse_array = []\n",
    "    for i in range(0,num_epochs):\n",
    "        predictions = get_predictions(VALIDATION_DATA.x,parameters[i])\n",
    "        mse = get_mean_squared_error(VALIDATION_DATA.y,predictions)\n",
    "        mse_array.append(mse)\n",
    "        epochs.append(i+1)\n",
    "\n",
    "    print(\"The MSE for the last epoch of {} is {}\".format(num_epochs,mse_array[-1]))\n",
    "\n",
    "    plt.plot(epochs,mse_array,'b-')\n",
    "    plt.suptitle(\"Mean Squared Error For Parameters Generated at Each Epoch\")\n",
    "    plt.xlabel(\"Epoch Number\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.show()\n",
    "\n",
    "def run_q1_b():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    step_size = 10 ** (-6)\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "    epochs = []\n",
    "    training_mse_array = []\n",
    "    validation_mse_array = []\n",
    "    for i in range(0, num_epochs):\n",
    "        training_predictions = get_predictions(TRAINING_DATA.x,parameters[i])\n",
    "        validation_predictions = get_predictions(VALIDATION_DATA.x, parameters[i])\n",
    "\n",
    "        training_mse = get_mean_squared_error(TRAINING_DATA.y,training_predictions)\n",
    "        validation_mse = get_mean_squared_error(VALIDATION_DATA.y, validation_predictions)\n",
    "\n",
    "        training_mse_array.append(training_mse)\n",
    "        validation_mse_array.append(validation_mse)\n",
    "\n",
    "        epochs.append(i + 1)\n",
    "\n",
    "    plt1, = plt.plot(epochs, training_mse_array, 'b-',label=\"Training MSE\")\n",
    "    plt2, = plt.plot(epochs,validation_mse_array,'r-',label=\"Validation MSE\")\n",
    "    plt.suptitle(\"Mean Squared Error For Parameters Generated at Each Epoch\")\n",
    "    plt.xlabel(\"Epoch Number\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def run_q2_a():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    for power in range(0,10):\n",
    "        step_size = 10**(-power)\n",
    "        print(\"Setting step size to {}\".format(step_size))\n",
    "\n",
    "        parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "        final_parameters = parameters[-1]\n",
    "        predictions = get_predictions(VALIDATION_DATA.x,final_parameters)\n",
    "        mse = get_mean_squared_error(VALIDATION_DATA.y,predictions)\n",
    "\n",
    "        print(\"For step size {}, after {} epochs, the MSE with Validation Data is {}\".format(step_size, num_epochs, mse))\n",
    "\n",
    "def run_q2_b():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    num_epochs = 10000\n",
    "    step_size = 0.01\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "    print(\"The step size is set to {}\".format(step_size))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                         parameters=initial_params,\n",
    "                                         step_size=step_size,\n",
    "                                         num_epochs=num_epochs)\n",
    "\n",
    "    final_parameters = parameters[-1]\n",
    "    predictions = get_predictions(TEST_DATA.x,final_parameters)\n",
    "    mse = get_mean_squared_error(TEST_DATA.y,predictions)\n",
    "\n",
    "    print(\"For step size {}, after {} epochs, the MSE with Test Data is {}\".format(step_size, num_epochs, mse))\n",
    "\n",
    "def run_q3():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    step_size = 0.01\n",
    "    num_epochs = 10000\n",
    "    range = [0,1.5]\n",
    "    num_sample_points = 150\n",
    "    random_5_epochs = (10,500,2000,5000,9000)\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "    plot_index = 321\n",
    "    for epoch in random_5_epochs:\n",
    "        plt.subplot(plot_index)\n",
    "        plt1, = plt.plot(TEST_DATA.x,TEST_DATA.y,'r.',label=\"Test Data\")\n",
    "\n",
    "        best_fit_x = np.linspace(range[0],range[1],num_sample_points)\n",
    "        best_fit_y = get_polynomial_output(best_fit_x,parameters[epoch+1])\n",
    "\n",
    "        plt2, = plt.plot(best_fit_x,best_fit_y,'b-',label=\"Regression Fit\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title(\"Fit for Epoch {}\".format(epoch))\n",
    "\n",
    "        plot_index+=1\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.4,hspace=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_q1_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been initialized: \n",
      "The initial parameters are set to (0.07642092956664504,0.80397616724715) \n",
      "The step size is set to 1e-06\n",
      "The number of epochs is set to 10000 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self,inp,out):\n",
    "        self.x = inp\n",
    "        self.y = out\n",
    "\n",
    "\n",
    "def get_mean_squared_error(target_values, predictions):\n",
    "    output = 0\n",
    "    num_predictions = len(predictions)\n",
    "    for i in range(0, num_predictions):\n",
    "        output += ((target_values[i] - predictions[i]) ** 2)\n",
    "\n",
    "    return (output / num_predictions)\n",
    "\n",
    "\n",
    "def get_polynomial_output(x,coefficients):\n",
    "    '''\n",
    "    Returns the output of the polynomial for a given x and its coefficients.\n",
    "    The coefficients must be in ascending order\n",
    "    '''\n",
    "    degree = len(coefficients)\n",
    "\n",
    "    output = 0\n",
    "    for power in range(0, degree):\n",
    "        output += coefficients[power] * (x ** power)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_predictions(inputs,coefficients):\n",
    "    predictions = []\n",
    "    for input in inputs:\n",
    "        predictions.append(get_polynomial_output(input,coefficients))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(dataset,parameters,step_size,num_epochs):\n",
    "    param_data = []\n",
    "\n",
    "    num_points = len(dataset.x)\n",
    "    data_array = np.zeros((num_points,2))       #numpy 2D array to hold given dataset\n",
    "\n",
    "    #This is a N*2 matrix where N is the number of data points. Each row contains the x and y values in that order\n",
    "    for point in range(0,num_points):\n",
    "        data_array[point][0] = dataset.x[point]\n",
    "        data_array[point][1] = dataset.y[point]\n",
    "\n",
    "    for i in range(0,num_epochs):\n",
    "        temp_data_array = np.copy(data_array)\n",
    "        np.random.shuffle(temp_data_array)\n",
    "        for j in range(0,num_points):\n",
    "            x = temp_data_array[j][0]\n",
    "            y = temp_data_array[j][1]\n",
    "            prediction = get_predictions([x],parameters)[0]\n",
    "\n",
    "            parameters[0] = parameters[0] - step_size*(prediction - y)\n",
    "            parameters[1] = parameters[1] - step_size*(prediction - y)*x\n",
    "\n",
    "        param_data.append([parameters[0], parameters[1]])\n",
    "    return param_data\n",
    "\n",
    "\n",
    "def initialize_data():\n",
    "    '''\n",
    "    This method calls the read_data method and initializes the data into the empty variables\n",
    "    created at the beginning of the program\n",
    "    '''\n",
    "    global TRAINING_DATA\n",
    "    TRAINING_DATA = read_data(\"Datasets/Dataset_2_train.csv\")\n",
    "    global VALIDATION_DATA\n",
    "    VALIDATION_DATA = read_data(\"Datasets/Dataset_2_valid.csv\")\n",
    "    global TEST_DATA\n",
    "    TEST_DATA = read_data(\"Datasets/Dataset_2_test.csv\")\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    inp = []\n",
    "    out = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            temp_line = line.split(',')\n",
    "            inp.append(float(temp_line[0]))\n",
    "            out.append(float(temp_line[1]))\n",
    "    return DataSet(inp, out)\n",
    "\n",
    "def run_q1_a():\n",
    "    initialize_data()\n",
    "    initial_params = [random(),random()]\n",
    "    step_size = 10**(-6)\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0],initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                parameters=initial_params,\n",
    "                                step_size=step_size,\n",
    "                                num_epochs=num_epochs)\n",
    "\n",
    "    epochs = []\n",
    "    mse_array = []\n",
    "    for i in range(0,num_epochs):\n",
    "        predictions = get_predictions(VALIDATION_DATA.x,parameters[i])\n",
    "        mse = get_mean_squared_error(VALIDATION_DATA.y,predictions)\n",
    "        mse_array.append(mse)\n",
    "        epochs.append(i+1)\n",
    "\n",
    "    print(\"The MSE for the last epoch of {} is {}\".format(num_epochs,mse_array[-1]))\n",
    "\n",
    "    plt.plot(epochs,mse_array,'b-')\n",
    "    plt.suptitle(\"Mean Squared Error For Parameters Generated at Each Epoch\")\n",
    "    plt.xlabel(\"Epoch Number\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.show()\n",
    "\n",
    "def run_q1_b():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    step_size = 10 ** (-6)\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "    epochs = []\n",
    "    training_mse_array = []\n",
    "    validation_mse_array = []\n",
    "    for i in range(0, num_epochs):\n",
    "        training_predictions = get_predictions(TRAINING_DATA.x,parameters[i])\n",
    "        validation_predictions = get_predictions(VALIDATION_DATA.x, parameters[i])\n",
    "\n",
    "        training_mse = get_mean_squared_error(TRAINING_DATA.y,training_predictions)\n",
    "        validation_mse = get_mean_squared_error(VALIDATION_DATA.y, validation_predictions)\n",
    "\n",
    "        training_mse_array.append(training_mse)\n",
    "        validation_mse_array.append(validation_mse)\n",
    "\n",
    "        epochs.append(i + 1)\n",
    "\n",
    "    plt1, = plt.plot(epochs, training_mse_array, 'b-',label=\"Training MSE\")\n",
    "    plt2, = plt.plot(epochs,validation_mse_array,'r-',label=\"Validation MSE\")\n",
    "    plt.suptitle(\"Mean Squared Error For Parameters Generated at Each Epoch\")\n",
    "    plt.xlabel(\"Epoch Number\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def run_q2_a():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    for power in range(0,10):\n",
    "        step_size = 10**(-power)\n",
    "        print(\"Setting step size to {}\".format(step_size))\n",
    "\n",
    "        parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "        final_parameters = parameters[-1]\n",
    "        predictions = get_predictions(VALIDATION_DATA.x,final_parameters)\n",
    "        mse = get_mean_squared_error(VALIDATION_DATA.y,predictions)\n",
    "\n",
    "        print(\"For step size {}, after {} epochs, the MSE with Validation Data is {}\".format(step_size, num_epochs, mse))\n",
    "\n",
    "def run_q2_b():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    num_epochs = 10000\n",
    "    step_size = 0.01\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "    print(\"The step size is set to {}\".format(step_size))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                         parameters=initial_params,\n",
    "                                         step_size=step_size,\n",
    "                                         num_epochs=num_epochs)\n",
    "\n",
    "    final_parameters = parameters[-1]\n",
    "    predictions = get_predictions(TEST_DATA.x,final_parameters)\n",
    "    mse = get_mean_squared_error(TEST_DATA.y,predictions)\n",
    "\n",
    "    print(\"For step size {}, after {} epochs, the MSE with Test Data is {}\".format(step_size, num_epochs, mse))\n",
    "\n",
    "def run_q3():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    step_size = 0.01\n",
    "    num_epochs = 10000\n",
    "    range = [0,1.5]\n",
    "    num_sample_points = 150\n",
    "    random_5_epochs = (10,500,2000,5000,9000)\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "    plot_index = 321\n",
    "    for epoch in random_5_epochs:\n",
    "        plt.subplot(plot_index)\n",
    "        plt1, = plt.plot(TEST_DATA.x,TEST_DATA.y,'r.',label=\"Test Data\")\n",
    "\n",
    "        best_fit_x = np.linspace(range[0],range[1],num_sample_points)\n",
    "        best_fit_y = get_polynomial_output(best_fit_x,parameters[epoch+1])\n",
    "\n",
    "        plt2, = plt.plot(best_fit_x,best_fit_y,'b-',label=\"Regression Fit\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title(\"Fit for Epoch {}\".format(epoch))\n",
    "\n",
    "        plot_index+=1\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.4,hspace=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_q1_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2)\n",
    "##### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been initialized: \n",
      "The initial parameters are set to (0.8060873905400328,0.20624270313941784) \n",
      "The number of epochs is set to 10000 \n",
      "Setting step size to 1\n",
      "For step size 1, after 10000 epochs, the MSE with Validation Data is 0.9327286933167278\n",
      "Setting step size to 0.1\n",
      "For step size 0.1, after 10000 epochs, the MSE with Validation Data is 0.10295043023432919\n",
      "Setting step size to 0.01\n",
      "For step size 0.01, after 10000 epochs, the MSE with Validation Data is 0.07383937311188904\n",
      "Setting step size to 0.001\n",
      "For step size 0.001, after 10000 epochs, the MSE with Validation Data is 0.07407900760914185\n",
      "Setting step size to 0.0001\n",
      "For step size 0.0001, after 10000 epochs, the MSE with Validation Data is 0.07407076424364561\n",
      "Setting step size to 1e-05\n",
      "For step size 1e-05, after 10000 epochs, the MSE with Validation Data is 0.07407031505064336\n",
      "Setting step size to 1e-06\n",
      "For step size 1e-06, after 10000 epochs, the MSE with Validation Data is 0.07407037629133449\n",
      "Setting step size to 1e-07\n",
      "For step size 1e-07, after 10000 epochs, the MSE with Validation Data is 0.0740703773280518\n",
      "Setting step size to 1e-08\n",
      "For step size 1e-08, after 10000 epochs, the MSE with Validation Data is 0.07407037741721423\n",
      "Setting step size to 1e-09\n",
      "For step size 1e-09, after 10000 epochs, the MSE with Validation Data is 0.0740703774257216\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self,inp,out):\n",
    "        self.x = inp\n",
    "        self.y = out\n",
    "\n",
    "\n",
    "def get_mean_squared_error(target_values, predictions):\n",
    "    output = 0\n",
    "    num_predictions = len(predictions)\n",
    "    for i in range(0, num_predictions):\n",
    "        output += ((target_values[i] - predictions[i]) ** 2)\n",
    "\n",
    "    return (output / num_predictions)\n",
    "\n",
    "\n",
    "def get_polynomial_output(x,coefficients):\n",
    "    '''\n",
    "    Returns the output of the polynomial for a given x and its coefficients.\n",
    "    The coefficients must be in ascending order\n",
    "    '''\n",
    "    degree = len(coefficients)\n",
    "\n",
    "    output = 0\n",
    "    for power in range(0, degree):\n",
    "        output += coefficients[power] * (x ** power)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_predictions(inputs,coefficients):\n",
    "    predictions = []\n",
    "    for input in inputs:\n",
    "        predictions.append(get_polynomial_output(input,coefficients))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(dataset,parameters,step_size,num_epochs):\n",
    "    param_data = []\n",
    "\n",
    "    num_points = len(dataset.x)\n",
    "    data_array = np.zeros((num_points,2))       #numpy 2D array to hold given dataset\n",
    "\n",
    "    #This is a N*2 matrix where N is the number of data points. Each row contains the x and y values in that order\n",
    "    for point in range(0,num_points):\n",
    "        data_array[point][0] = dataset.x[point]\n",
    "        data_array[point][1] = dataset.y[point]\n",
    "\n",
    "    for i in range(0,num_epochs):\n",
    "        temp_data_array = np.copy(data_array)\n",
    "        np.random.shuffle(temp_data_array)\n",
    "        for j in range(0,num_points):\n",
    "            x = temp_data_array[j][0]\n",
    "            y = temp_data_array[j][1]\n",
    "            prediction = get_predictions([x],parameters)[0]\n",
    "\n",
    "            parameters[0] = parameters[0] - step_size*(prediction - y)\n",
    "            parameters[1] = parameters[1] - step_size*(prediction - y)*x\n",
    "\n",
    "        param_data.append([parameters[0], parameters[1]])\n",
    "    return param_data\n",
    "\n",
    "\n",
    "def initialize_data():\n",
    "    '''\n",
    "    This method calls the read_data method and initializes the data into the empty variables\n",
    "    created at the beginning of the program\n",
    "    '''\n",
    "    global TRAINING_DATA\n",
    "    TRAINING_DATA = read_data(\"Datasets/Dataset_2_train.csv\")\n",
    "    global VALIDATION_DATA\n",
    "    VALIDATION_DATA = read_data(\"Datasets/Dataset_2_valid.csv\")\n",
    "    global TEST_DATA\n",
    "    TEST_DATA = read_data(\"Datasets/Dataset_2_test.csv\")\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    inp = []\n",
    "    out = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            temp_line = line.split(',')\n",
    "            inp.append(float(temp_line[0]))\n",
    "            out.append(float(temp_line[1]))\n",
    "    return DataSet(inp, out)\n",
    "\n",
    "def run_q1_a():\n",
    "    initialize_data()\n",
    "    initial_params = [random(),random()]\n",
    "    step_size = 10**(-6)\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0],initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                parameters=initial_params,\n",
    "                                step_size=step_size,\n",
    "                                num_epochs=num_epochs)\n",
    "\n",
    "    epochs = []\n",
    "    mse_array = []\n",
    "    for i in range(0,num_epochs):\n",
    "        predictions = get_predictions(VALIDATION_DATA.x,parameters[i])\n",
    "        mse = get_mean_squared_error(VALIDATION_DATA.y,predictions)\n",
    "        mse_array.append(mse)\n",
    "        epochs.append(i+1)\n",
    "\n",
    "    print(\"The MSE for the last epoch of {} is {}\".format(num_epochs,mse_array[-1]))\n",
    "\n",
    "    plt.plot(epochs,mse_array,'b-')\n",
    "    plt.suptitle(\"Mean Squared Error For Parameters Generated at Each Epoch\")\n",
    "    plt.xlabel(\"Epoch Number\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.show()\n",
    "\n",
    "def run_q1_b():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    step_size = 10 ** (-6)\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "    epochs = []\n",
    "    training_mse_array = []\n",
    "    validation_mse_array = []\n",
    "    for i in range(0, num_epochs):\n",
    "        training_predictions = get_predictions(TRAINING_DATA.x,parameters[i])\n",
    "        validation_predictions = get_predictions(VALIDATION_DATA.x, parameters[i])\n",
    "\n",
    "        training_mse = get_mean_squared_error(TRAINING_DATA.y,training_predictions)\n",
    "        validation_mse = get_mean_squared_error(VALIDATION_DATA.y, validation_predictions)\n",
    "\n",
    "        training_mse_array.append(training_mse)\n",
    "        validation_mse_array.append(validation_mse)\n",
    "\n",
    "        epochs.append(i + 1)\n",
    "\n",
    "    plt1, = plt.plot(epochs, training_mse_array, 'b-',label=\"Training MSE\")\n",
    "    plt2, = plt.plot(epochs,validation_mse_array,'r-',label=\"Validation MSE\")\n",
    "    plt.suptitle(\"Mean Squared Error For Parameters Generated at Each Epoch\")\n",
    "    plt.xlabel(\"Epoch Number\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def run_q2_a():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    for power in range(0,10):\n",
    "        step_size = 10**(-power)\n",
    "        print(\"Setting step size to {}\".format(step_size))\n",
    "\n",
    "        parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "        final_parameters = parameters[-1]\n",
    "        predictions = get_predictions(VALIDATION_DATA.x,final_parameters)\n",
    "        mse = get_mean_squared_error(VALIDATION_DATA.y,predictions)\n",
    "\n",
    "        print(\"For step size {}, after {} epochs, the MSE with Validation Data is {}\".format(step_size, num_epochs, mse))\n",
    "\n",
    "def run_q2_b():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    num_epochs = 10000\n",
    "    step_size = 0.01\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "    print(\"The step size is set to {}\".format(step_size))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                         parameters=initial_params,\n",
    "                                         step_size=step_size,\n",
    "                                         num_epochs=num_epochs)\n",
    "\n",
    "    final_parameters = parameters[-1]\n",
    "    predictions = get_predictions(TEST_DATA.x,final_parameters)\n",
    "    mse = get_mean_squared_error(TEST_DATA.y,predictions)\n",
    "\n",
    "    print(\"For step size {}, after {} epochs, the MSE with Test Data is {}\".format(step_size, num_epochs, mse))\n",
    "\n",
    "def run_q3():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    step_size = 0.01\n",
    "    num_epochs = 10000\n",
    "    range = [0,1.5]\n",
    "    num_sample_points = 150\n",
    "    random_5_epochs = (10,500,2000,5000,9000)\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "    plot_index = 321\n",
    "    for epoch in random_5_epochs:\n",
    "        plt.subplot(plot_index)\n",
    "        plt1, = plt.plot(TEST_DATA.x,TEST_DATA.y,'r.',label=\"Test Data\")\n",
    "\n",
    "        best_fit_x = np.linspace(range[0],range[1],num_sample_points)\n",
    "        best_fit_y = get_polynomial_output(best_fit_x,parameters[epoch+1])\n",
    "\n",
    "        plt2, = plt.plot(best_fit_x,best_fit_y,'b-',label=\"Regression Fit\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title(\"Fit for Epoch {}\".format(epoch))\n",
    "\n",
    "        plot_index+=1\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.4,hspace=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_q2_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been initialized: \n",
      "The initial parameters are set to (0.12762137569364174,0.8825862396149773) \n",
      "The number of epochs is set to 10000 \n",
      "The step size is set to 0.01\n",
      "For step size 0.01, after 10000 epochs, the MSE with Test Data is 0.07088041944135273\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self,inp,out):\n",
    "        self.x = inp\n",
    "        self.y = out\n",
    "\n",
    "\n",
    "def get_mean_squared_error(target_values, predictions):\n",
    "    output = 0\n",
    "    num_predictions = len(predictions)\n",
    "    for i in range(0, num_predictions):\n",
    "        output += ((target_values[i] - predictions[i]) ** 2)\n",
    "\n",
    "    return (output / num_predictions)\n",
    "\n",
    "\n",
    "def get_polynomial_output(x,coefficients):\n",
    "    '''\n",
    "    Returns the output of the polynomial for a given x and its coefficients.\n",
    "    The coefficients must be in ascending order\n",
    "    '''\n",
    "    degree = len(coefficients)\n",
    "\n",
    "    output = 0\n",
    "    for power in range(0, degree):\n",
    "        output += coefficients[power] * (x ** power)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_predictions(inputs,coefficients):\n",
    "    predictions = []\n",
    "    for input in inputs:\n",
    "        predictions.append(get_polynomial_output(input,coefficients))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(dataset,parameters,step_size,num_epochs):\n",
    "    param_data = []\n",
    "\n",
    "    num_points = len(dataset.x)\n",
    "    data_array = np.zeros((num_points,2))       #numpy 2D array to hold given dataset\n",
    "\n",
    "    #This is a N*2 matrix where N is the number of data points. Each row contains the x and y values in that order\n",
    "    for point in range(0,num_points):\n",
    "        data_array[point][0] = dataset.x[point]\n",
    "        data_array[point][1] = dataset.y[point]\n",
    "\n",
    "    for i in range(0,num_epochs):\n",
    "        temp_data_array = np.copy(data_array)\n",
    "        np.random.shuffle(temp_data_array)\n",
    "        for j in range(0,num_points):\n",
    "            x = temp_data_array[j][0]\n",
    "            y = temp_data_array[j][1]\n",
    "            prediction = get_predictions([x],parameters)[0]\n",
    "\n",
    "            parameters[0] = parameters[0] - step_size*(prediction - y)\n",
    "            parameters[1] = parameters[1] - step_size*(prediction - y)*x\n",
    "\n",
    "        param_data.append([parameters[0], parameters[1]])\n",
    "    return param_data\n",
    "\n",
    "\n",
    "def initialize_data():\n",
    "    '''\n",
    "    This method calls the read_data method and initializes the data into the empty variables\n",
    "    created at the beginning of the program\n",
    "    '''\n",
    "    global TRAINING_DATA\n",
    "    TRAINING_DATA = read_data(\"Datasets/Dataset_2_train.csv\")\n",
    "    global VALIDATION_DATA\n",
    "    VALIDATION_DATA = read_data(\"Datasets/Dataset_2_valid.csv\")\n",
    "    global TEST_DATA\n",
    "    TEST_DATA = read_data(\"Datasets/Dataset_2_test.csv\")\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    inp = []\n",
    "    out = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            temp_line = line.split(',')\n",
    "            inp.append(float(temp_line[0]))\n",
    "            out.append(float(temp_line[1]))\n",
    "    return DataSet(inp, out)\n",
    "\n",
    "def run_q1_a():\n",
    "    initialize_data()\n",
    "    initial_params = [random(),random()]\n",
    "    step_size = 10**(-6)\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0],initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                parameters=initial_params,\n",
    "                                step_size=step_size,\n",
    "                                num_epochs=num_epochs)\n",
    "\n",
    "    epochs = []\n",
    "    mse_array = []\n",
    "    for i in range(0,num_epochs):\n",
    "        predictions = get_predictions(VALIDATION_DATA.x,parameters[i])\n",
    "        mse = get_mean_squared_error(VALIDATION_DATA.y,predictions)\n",
    "        mse_array.append(mse)\n",
    "        epochs.append(i+1)\n",
    "\n",
    "    print(\"The MSE for the last epoch of {} is {}\".format(num_epochs,mse_array[-1]))\n",
    "\n",
    "    plt.plot(epochs,mse_array,'b-')\n",
    "    plt.suptitle(\"Mean Squared Error For Parameters Generated at Each Epoch\")\n",
    "    plt.xlabel(\"Epoch Number\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.show()\n",
    "\n",
    "def run_q1_b():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    step_size = 10 ** (-6)\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "    epochs = []\n",
    "    training_mse_array = []\n",
    "    validation_mse_array = []\n",
    "    for i in range(0, num_epochs):\n",
    "        training_predictions = get_predictions(TRAINING_DATA.x,parameters[i])\n",
    "        validation_predictions = get_predictions(VALIDATION_DATA.x, parameters[i])\n",
    "\n",
    "        training_mse = get_mean_squared_error(TRAINING_DATA.y,training_predictions)\n",
    "        validation_mse = get_mean_squared_error(VALIDATION_DATA.y, validation_predictions)\n",
    "\n",
    "        training_mse_array.append(training_mse)\n",
    "        validation_mse_array.append(validation_mse)\n",
    "\n",
    "        epochs.append(i + 1)\n",
    "\n",
    "    plt1, = plt.plot(epochs, training_mse_array, 'b-',label=\"Training MSE\")\n",
    "    plt2, = plt.plot(epochs,validation_mse_array,'r-',label=\"Validation MSE\")\n",
    "    plt.suptitle(\"Mean Squared Error For Parameters Generated at Each Epoch\")\n",
    "    plt.xlabel(\"Epoch Number\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def run_q2_a():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    for power in range(0,10):\n",
    "        step_size = 10**(-power)\n",
    "        print(\"Setting step size to {}\".format(step_size))\n",
    "\n",
    "        parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "        final_parameters = parameters[-1]\n",
    "        predictions = get_predictions(VALIDATION_DATA.x,final_parameters)\n",
    "        mse = get_mean_squared_error(VALIDATION_DATA.y,predictions)\n",
    "\n",
    "        print(\"For step size {}, after {} epochs, the MSE with Validation Data is {}\".format(step_size, num_epochs, mse))\n",
    "\n",
    "def run_q2_b():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    num_epochs = 10000\n",
    "    step_size = 0.01\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "    print(\"The step size is set to {}\".format(step_size))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                         parameters=initial_params,\n",
    "                                         step_size=step_size,\n",
    "                                         num_epochs=num_epochs)\n",
    "\n",
    "    final_parameters = parameters[-1]\n",
    "    predictions = get_predictions(TEST_DATA.x,final_parameters)\n",
    "    mse = get_mean_squared_error(TEST_DATA.y,predictions)\n",
    "\n",
    "    print(\"For step size {}, after {} epochs, the MSE with Test Data is {}\".format(step_size, num_epochs, mse))\n",
    "\n",
    "def run_q3():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    step_size = 0.01\n",
    "    num_epochs = 10000\n",
    "    range = [0,1.5]\n",
    "    num_sample_points = 150\n",
    "    random_5_epochs = (10,500,2000,5000,9000)\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "    plot_index = 321\n",
    "    for epoch in random_5_epochs:\n",
    "        plt.subplot(plot_index)\n",
    "        plt1, = plt.plot(TEST_DATA.x,TEST_DATA.y,'r.',label=\"Test Data\")\n",
    "\n",
    "        best_fit_x = np.linspace(range[0],range[1],num_sample_points)\n",
    "        best_fit_y = get_polynomial_output(best_fit_x,parameters[epoch+1])\n",
    "\n",
    "        plt2, = plt.plot(best_fit_x,best_fit_y,'b-',label=\"Regression Fit\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title(\"Fit for Epoch {}\".format(epoch))\n",
    "\n",
    "        plot_index+=1\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.4,hspace=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_q2_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been initialized: \n",
      "The initial parameters are set to (0.5430241351980879,0.9957620591822134) \n",
      "The step size is set to 0.01\n",
      "The number of epochs is set to 10000 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "TRAINING_DATA = None\n",
    "VALIDATION_DATA = None\n",
    "TEST_DATA = None\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self,inp,out):\n",
    "        self.x = inp\n",
    "        self.y = out\n",
    "\n",
    "\n",
    "def get_mean_squared_error(target_values, predictions):\n",
    "    output = 0\n",
    "    num_predictions = len(predictions)\n",
    "    for i in range(0, num_predictions):\n",
    "        output += ((target_values[i] - predictions[i]) ** 2)\n",
    "\n",
    "    return (output / num_predictions)\n",
    "\n",
    "\n",
    "def get_polynomial_output(x,coefficients):\n",
    "    '''\n",
    "    Returns the output of the polynomial for a given x and its coefficients.\n",
    "    The coefficients must be in ascending order\n",
    "    '''\n",
    "    degree = len(coefficients)\n",
    "\n",
    "    output = 0\n",
    "    for power in range(0, degree):\n",
    "        output += coefficients[power] * (x ** power)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_predictions(inputs,coefficients):\n",
    "    predictions = []\n",
    "    for input in inputs:\n",
    "        predictions.append(get_polynomial_output(input,coefficients))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(dataset,parameters,step_size,num_epochs):\n",
    "    param_data = []\n",
    "\n",
    "    num_points = len(dataset.x)\n",
    "    data_array = np.zeros((num_points,2))       #numpy 2D array to hold given dataset\n",
    "\n",
    "    #This is a N*2 matrix where N is the number of data points. Each row contains the x and y values in that order\n",
    "    for point in range(0,num_points):\n",
    "        data_array[point][0] = dataset.x[point]\n",
    "        data_array[point][1] = dataset.y[point]\n",
    "\n",
    "    for i in range(0,num_epochs):\n",
    "        temp_data_array = np.copy(data_array)\n",
    "        np.random.shuffle(temp_data_array)\n",
    "        for j in range(0,num_points):\n",
    "            x = temp_data_array[j][0]\n",
    "            y = temp_data_array[j][1]\n",
    "            prediction = get_predictions([x],parameters)[0]\n",
    "\n",
    "            parameters[0] = parameters[0] - step_size*(prediction - y)\n",
    "            parameters[1] = parameters[1] - step_size*(prediction - y)*x\n",
    "\n",
    "        param_data.append([parameters[0], parameters[1]])\n",
    "    return param_data\n",
    "\n",
    "\n",
    "def initialize_data():\n",
    "    '''\n",
    "    This method calls the read_data method and initializes the data into the empty variables\n",
    "    created at the beginning of the program\n",
    "    '''\n",
    "    global TRAINING_DATA\n",
    "    TRAINING_DATA = read_data(\"Datasets/Dataset_2_train.csv\")\n",
    "    global VALIDATION_DATA\n",
    "    VALIDATION_DATA = read_data(\"Datasets/Dataset_2_valid.csv\")\n",
    "    global TEST_DATA\n",
    "    TEST_DATA = read_data(\"Datasets/Dataset_2_test.csv\")\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    inp = []\n",
    "    out = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            temp_line = line.split(',')\n",
    "            inp.append(float(temp_line[0]))\n",
    "            out.append(float(temp_line[1]))\n",
    "    return DataSet(inp, out)\n",
    "\n",
    "def run_q1_a():\n",
    "    initialize_data()\n",
    "    initial_params = [random(),random()]\n",
    "    step_size = 10**(-6)\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0],initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                parameters=initial_params,\n",
    "                                step_size=step_size,\n",
    "                                num_epochs=num_epochs)\n",
    "\n",
    "    epochs = []\n",
    "    mse_array = []\n",
    "    for i in range(0,num_epochs):\n",
    "        predictions = get_predictions(VALIDATION_DATA.x,parameters[i])\n",
    "        mse = get_mean_squared_error(VALIDATION_DATA.y,predictions)\n",
    "        mse_array.append(mse)\n",
    "        epochs.append(i+1)\n",
    "\n",
    "    print(\"The MSE for the last epoch of {} is {}\".format(num_epochs,mse_array[-1]))\n",
    "\n",
    "    plt.plot(epochs,mse_array,'b-')\n",
    "    plt.suptitle(\"Mean Squared Error For Parameters Generated at Each Epoch\")\n",
    "    plt.xlabel(\"Epoch Number\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.show()\n",
    "\n",
    "def run_q1_b():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    step_size = 10 ** (-6)\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "    epochs = []\n",
    "    training_mse_array = []\n",
    "    validation_mse_array = []\n",
    "    for i in range(0, num_epochs):\n",
    "        training_predictions = get_predictions(TRAINING_DATA.x,parameters[i])\n",
    "        validation_predictions = get_predictions(VALIDATION_DATA.x, parameters[i])\n",
    "\n",
    "        training_mse = get_mean_squared_error(TRAINING_DATA.y,training_predictions)\n",
    "        validation_mse = get_mean_squared_error(VALIDATION_DATA.y, validation_predictions)\n",
    "\n",
    "        training_mse_array.append(training_mse)\n",
    "        validation_mse_array.append(validation_mse)\n",
    "\n",
    "        epochs.append(i + 1)\n",
    "\n",
    "    plt1, = plt.plot(epochs, training_mse_array, 'b-',label=\"Training MSE\")\n",
    "    plt2, = plt.plot(epochs,validation_mse_array,'r-',label=\"Validation MSE\")\n",
    "    plt.suptitle(\"Mean Squared Error For Parameters Generated at Each Epoch\")\n",
    "    plt.xlabel(\"Epoch Number\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def run_q2_a():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    num_epochs = 10000\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    for power in range(0,10):\n",
    "        step_size = 10**(-power)\n",
    "        print(\"Setting step size to {}\".format(step_size))\n",
    "\n",
    "        parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "        final_parameters = parameters[-1]\n",
    "        predictions = get_predictions(VALIDATION_DATA.x,final_parameters)\n",
    "        mse = get_mean_squared_error(VALIDATION_DATA.y,predictions)\n",
    "\n",
    "        print(\"For step size {}, after {} epochs, the MSE with Validation Data is {}\".format(step_size, num_epochs, mse))\n",
    "\n",
    "def run_q2_b():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    num_epochs = 10000\n",
    "    step_size = 0.01\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "    print(\"The step size is set to {}\".format(step_size))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                         parameters=initial_params,\n",
    "                                         step_size=step_size,\n",
    "                                         num_epochs=num_epochs)\n",
    "\n",
    "    final_parameters = parameters[-1]\n",
    "    predictions = get_predictions(TEST_DATA.x,final_parameters)\n",
    "    mse = get_mean_squared_error(TEST_DATA.y,predictions)\n",
    "\n",
    "    print(\"For step size {}, after {} epochs, the MSE with Test Data is {}\".format(step_size, num_epochs, mse))\n",
    "\n",
    "def run_q3():\n",
    "    initialize_data()\n",
    "    initial_params = [random(), random()]\n",
    "    step_size = 0.01\n",
    "    num_epochs = 10000\n",
    "    range = [0,1.5]\n",
    "    num_sample_points = 150\n",
    "    random_5_epochs = (10,500,2000,5000,9000)\n",
    "\n",
    "    print(\"The data has been initialized: \")\n",
    "    print(\"The initial parameters are set to ({},{}) \".format(initial_params[0], initial_params[1]))\n",
    "    print(\"The step size is set to {}\".format((step_size)))\n",
    "    print(\"The number of epochs is set to {} \".format(num_epochs))\n",
    "\n",
    "    parameters = stochastic_gradient_descent(dataset=TRAINING_DATA,\n",
    "                                             parameters=initial_params,\n",
    "                                             step_size=step_size,\n",
    "                                             num_epochs=num_epochs)\n",
    "\n",
    "    plot_index = 321\n",
    "    for epoch in random_5_epochs:\n",
    "        plt.subplot(plot_index)\n",
    "        plt1, = plt.plot(TEST_DATA.x,TEST_DATA.y,'r.',label=\"Test Data\")\n",
    "\n",
    "        best_fit_x = np.linspace(range[0],range[1],num_sample_points)\n",
    "        best_fit_y = get_polynomial_output(best_fit_x,parameters[epoch+1])\n",
    "\n",
    "        plt2, = plt.plot(best_fit_x,best_fit_y,'b-',label=\"Regression Fit\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title(\"Fit for Epoch {}\".format(epoch))\n",
    "\n",
    "        plot_index+=1\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.4,hspace=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_q3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part 4 - Real Life Dataset\n",
    " #### Q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing all ? with empty cells\n",
      "Updated file saved to Datasets/CrimeData/crime_data_refined.csv\n",
      "\n",
      "Replacing all missing values with mean\n",
      "Updated data saved to Datasets/CrimeData/crime_data_updated_mean.csv\n",
      "\n",
      "Removing non-predictor data columns\n",
      "Replacing all missing values with median\n",
      "Updated data saved to Datasets/CrimeData/crime_data_updated_custom.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import statistics\n",
    "\n",
    "def calculate_mean(data):\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    for e in data:\n",
    "        if e:\n",
    "            sum+=float(e)\n",
    "            count+=1\n",
    "    return sum/count\n",
    "\n",
    "def calculate_median(data):\n",
    "    refined_data = []\n",
    "    for e in data:\n",
    "        if e:\n",
    "            refined_data.append(float(e))\n",
    "    return statistics.median(refined_data)\n",
    "\n",
    "def refine_data(data_path,saving_path):\n",
    "    lines = None\n",
    "    output = \"\"\n",
    "    with open(data_path,'r') as f:\n",
    "        lines=f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        temp_list = line.split(',')\n",
    "        for i in range(0,len(temp_list)):\n",
    "            if temp_list[i] == \"?\":\n",
    "                temp_list[i] = \"\"\n",
    "        formatted_line = ','.join(temp_list)\n",
    "        output += formatted_line\n",
    "\n",
    "    with open(saving_path,'w') as f:\n",
    "        f.write(output)\n",
    "def update_missing_data_with_median(data_path,ignore_col_num=[]):\n",
    "    mean_dict = {}\n",
    "    cols = []\n",
    "    with open(data_path, 'r+') as f:\n",
    "        csv_data = csv.reader(f, delimiter=',')\n",
    "\n",
    "        count = 0\n",
    "        for row in csv_data:\n",
    "            count_2 = 0\n",
    "            for col in row:\n",
    "                if count == 0:\n",
    "                    cols.append([col])\n",
    "                else:\n",
    "                    cols[count_2].append(col)\n",
    "                count_2 += 1\n",
    "            count += 1\n",
    "\n",
    "    for i in range(0, len(cols)):\n",
    "        if i not in ignore_col_num:\n",
    "            mean_dict[i] = calculate_median(cols[i])\n",
    "\n",
    "    output = \"\"\n",
    "    with open(data_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        temp_list = line.split(',')\n",
    "        for i in range(0, len(temp_list)):\n",
    "            if temp_list[i] == \"\":\n",
    "                temp_list[i] = str(mean_dict[i])\n",
    "        formatted_line = ','.join(temp_list)\n",
    "        output += formatted_line\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def update_missing_data_with_mean(data_path,ignore_col_num=[]):\n",
    "    mean_dict = {}\n",
    "    cols = []\n",
    "    with open(data_path,'r+') as f:\n",
    "        csv_data = csv.reader(f,delimiter=',')\n",
    "\n",
    "        count = 0\n",
    "        for row in csv_data:\n",
    "            count_2 = 0\n",
    "            for col in row:\n",
    "                if count==0:\n",
    "                    cols.append([col])\n",
    "                else:\n",
    "                    cols[count_2].append(col)\n",
    "                count_2+=1\n",
    "            count+=1\n",
    "\n",
    "    for i in range(0,len(cols)):\n",
    "        if i not in ignore_col_num:\n",
    "            mean_dict[i] = calculate_mean(cols[i])\n",
    "\n",
    "    output = \"\"\n",
    "    with open(data_path,'r') as f:\n",
    "        lines=f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        temp_list = line.split(',')\n",
    "        for i in range(0,len(temp_list)):\n",
    "            if temp_list[i] == \"\":\n",
    "                temp_list[i] = str(mean_dict[i])\n",
    "        formatted_line = ','.join(temp_list)\n",
    "        output += formatted_line\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def remove_useless_cols(data_path,useless_cols):\n",
    "    output = \"\"\n",
    "    with open(data_path,'r') as f:\n",
    "        lines=f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        temp_list=line.split(',')\n",
    "        formatted_list = []\n",
    "        for i in range(0,len(temp_list)):\n",
    "            if i not in useless_cols:\n",
    "                formatted_list.append(temp_list[i])\n",
    "        output+= (',').join(formatted_list)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def remove_missing_data_cols(data_path,removal_threshold=0.5):\n",
    "    '''\n",
    "    This method removes all variables that have instances greater that the removal threshold for missing data\n",
    "    and updates the rest of the missing data with median.\n",
    "    '''\n",
    "\n",
    "    with open(data_path,'r') as f:\n",
    "        lines=f.readlines()\n",
    "\n",
    "    missing_data_cols = {}\n",
    "    for i in range(0,len(lines[0].split(','))):\n",
    "        missing_data_cols[i] = 0\n",
    "\n",
    "    for line in lines:\n",
    "        temp_list = line.split(',')\n",
    "        for i in range(0,len(temp_list)):\n",
    "            if temp_list[i] == \"\":\n",
    "                missing_data_cols[i]+=1\n",
    "\n",
    "    total_features = len(lines)\n",
    "    removal_threshold = removal_threshold*total_features\n",
    "    output = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        temp_list=line.split(',')\n",
    "        formatted_list = []\n",
    "        for i in range(0,len(temp_list)):\n",
    "            if missing_data_cols[i] < removal_threshold:\n",
    "                formatted_list.append(temp_list[i])\n",
    "        output+= (',').join(formatted_list)\n",
    "\n",
    "    return output\n",
    "\n",
    "def remove_missing_data_rows(data_path):\n",
    "    '''\n",
    "    This method removes all rows that have any missing data\n",
    "    '''\n",
    "    output = \"\"\n",
    "    with open(data_path,'r') as f:\n",
    "        lines=f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        add_to_output = True\n",
    "        temp_list=line.split(',')\n",
    "        for i in range(0,len(temp_list)):\n",
    "            if temp_list[i] == \"\":\n",
    "                add_to_output=False\n",
    "                break\n",
    "        if add_to_output:\n",
    "            output+=line\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    original_data_path = 'Datasets/CrimeData/crime_data.csv'\n",
    "    refined_data_path = 'Datasets/CrimeData/crime_data_refined.csv'   #path to data where ? are replaced with empty cells\n",
    "    update_path_mean = 'Datasets/CrimeData/crime_data_updated_mean.csv'   #path to data where all missing data is replaced with mean\n",
    "    update_path_custom = 'Datasets/CrimeData/crime_data_updated_custom.csv'\n",
    "\n",
    "    print(\"Replacing all ? with empty cells\")\n",
    "    refine_data(data_path=original_data_path,\n",
    "                saving_path=refined_data_path)\n",
    "    print(\"Updated file saved to {}\\n\".format(refined_data_path))\n",
    "\n",
    "    print(\"Replacing all missing values with mean\")\n",
    "    refined_data_mean = update_missing_data_with_mean(refined_data_path,ignore_col_num=[3])\n",
    "    with open(update_path_mean,'w') as f:\n",
    "        f.write(refined_data_mean)\n",
    "    print(\"Updated data saved to {}\\n\".format(update_path_mean))\n",
    "\n",
    "\n",
    "    print(\"Removing non-predictor data columns\")\n",
    "    refined_data_custom = remove_useless_cols(data_path=refined_data_path,useless_cols=[0,1,2,3,4])\n",
    "    with open(update_path_custom, 'w') as f:\n",
    "        f.write(refined_data_custom)\n",
    "\n",
    "    print(\"Replacing all missing values with median\")\n",
    "    refined_data_custom = update_missing_data_with_median(data_path=update_path_custom,ignore_col_num=[3])\n",
    "    with open(update_path_custom,'w') as f:\n",
    "        f.write(refined_data_custom)\n",
    "    print(\"Updated data saved to {}\\n\".format(update_path_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data from Datasets/CrimeData/crime_data_updated_custom.csv\n",
      "Generating 5 80-20 splits with the initial data\n",
      "The MSE for Dataset 1 is 0.020851013218439733\n",
      "The MSE for Dataset 2 is 0.018152452172590296\n",
      "The MSE for Dataset 3 is 0.019363586651732802\n",
      "The MSE for Dataset 4 is 0.017112754465998897\n",
      "The MSE for Dataset 5 is 0.01972137772502004\n",
      "The average MSE over these 5 Datasets is 0.019040236846756355\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "INITIAL_DATA = None\n",
    "\n",
    "def set_initial_data(data_path):\n",
    "    global INITIAL_DATA\n",
    "    with open(data_path,'r') as f:\n",
    "        INITIAL_DATA = f.readlines()\n",
    "\n",
    "def generate_80_20_splits(num_splits,saving_name='CandC'):\n",
    "    if not INITIAL_DATA:\n",
    "        print(\"Please set initial data first!\")\n",
    "        return None\n",
    "\n",
    "    split_20_num = int(len(INITIAL_DATA) * 0.2)\n",
    "\n",
    "    for i in range(1,num_splits+1):\n",
    "        temp_data = deepcopy(INITIAL_DATA)\n",
    "        test_split = \"\"\n",
    "        for j in range(0,split_20_num):\n",
    "            test_selection = random.randint(0,len(temp_data)-1)\n",
    "            test_split+= temp_data[test_selection]\n",
    "            temp_data.pop(test_selection)\n",
    "\n",
    "        train_split = \"\".join(temp_data)\n",
    "        with open(\"{}-train{}.csv\".format(saving_name,i),'w') as f:\n",
    "            f.write(train_split)\n",
    "        with open(\"{}-test{}.csv\".format(saving_name,i),'w') as f:\n",
    "            f.write(test_split)\n",
    "\n",
    "def split_x_y(data):\n",
    "    '''\n",
    "    Takes in a list of rows where all columns except last one are xi's\n",
    "    and splits them into separate vectors\n",
    "    '''\n",
    "\n",
    "    split_data = []\n",
    "\n",
    "    for line in data:\n",
    "        temp_line = line.split(',')\n",
    "        y_vector = [float(temp_line[-1])]\n",
    "        temp_line.pop(-1)\n",
    "        x_vector = [float(i) for i in temp_line]\n",
    "\n",
    "        split_data.append((x_vector,y_vector))\n",
    "\n",
    "    return split_data\n",
    "\n",
    "def extract_x_from_split_data(data):\n",
    "    x_matrix = []\n",
    "    for row in data:\n",
    "        x_matrix.append(row[0])\n",
    "    return x_matrix\n",
    "\n",
    "def extract_y_from_split_data(data):\n",
    "    y_matrix = []\n",
    "    for row in data:\n",
    "        y_matrix.append(row[1])\n",
    "    return y_matrix\n",
    "\n",
    "\n",
    "def get_best_fit(x_matrix,y_matrix):\n",
    "    '''\n",
    "    Using the initialized data set this method finds the best fit for the data.\n",
    "    It uses the least squares regression method and returns an array containing\n",
    "    all the coefficients found with this method.\n",
    "    '''\n",
    "\n",
    "    mpps_x = np.linalg.pinv(np.array(x_matrix))\n",
    "\n",
    "    return np.matmul(mpps_x,np.array(y_matrix))\n",
    "\n",
    "def get_predictions(coefficients,x_matrix):\n",
    "    predictions = []\n",
    "    for i in range(0,len(x_matrix)):\n",
    "        prediction = 0\n",
    "        for j in range(0,len(x_matrix[i])):\n",
    "            prediction += coefficients[j]*x_matrix[i][j]\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def get_mean_squared_error(target_values, predictions):\n",
    "    output = 0\n",
    "    num_predictions = len(predictions)\n",
    "\n",
    "    for i in range(0, num_predictions):\n",
    "        output += ((target_values[i] - predictions[i]) ** 2)\n",
    "\n",
    "    return (output / num_predictions)[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init_data_path = 'Datasets/CrimeData/crime_data_updated_custom.csv'\n",
    "    split_data_name = 'Datasets/CrimeData/CandC'\n",
    "    num_splits = 5\n",
    "\n",
    "    print(\"Initializing data from {}\".format(init_data_path))\n",
    "    set_initial_data(init_data_path)\n",
    "\n",
    "    print(\"Generating {} 80-20 splits with the initial data\".format(num_splits))\n",
    "    generate_80_20_splits(num_splits,saving_name=split_data_name)\n",
    "\n",
    "    total_mse = 0\n",
    "    for i in range(1,num_splits+1):\n",
    "        training_data = []\n",
    "        testing_data = []\n",
    "\n",
    "        with open(\"{}-train{}.csv\".format(split_data_name,i),'r') as f:\n",
    "            training_data = f.readlines()\n",
    "\n",
    "        with open(\"{}-test{}.csv\".format(split_data_name,i),'r') as f:\n",
    "            testing_data = f.readlines()\n",
    "\n",
    "        split_training_data = split_x_y(training_data)\n",
    "        split_testing_data = split_x_y(testing_data)\n",
    "\n",
    "        training_data_x = extract_x_from_split_data(split_training_data)\n",
    "        training_data_y = extract_y_from_split_data(split_training_data)\n",
    "        testing_data_x = extract_x_from_split_data(split_testing_data)\n",
    "        testing_data_y = extract_y_from_split_data(split_testing_data)\n",
    "\n",
    "        best_fit_coefficients = get_best_fit(training_data_x,training_data_y)\n",
    "        predictions = get_predictions(best_fit_coefficients,testing_data_x)\n",
    "        mse = get_mean_squared_error(testing_data_y,predictions)\n",
    "\n",
    "        total_mse += mse\n",
    "\n",
    "        print(\"The MSE for Dataset {} is {}\".format(i,mse))\n",
    "\n",
    "    print(\"The average MSE over these {} Datasets is {}\".format(num_splits,total_mse/num_splits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3)\n",
    "##### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data from Datasets/CrimeData/crime_data_updated_custom.csv\n",
      "Generating 5 80-20 splits with the initial data\n",
      "The smallest MSE 0.02002176625519709 was found for 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "INITIAL_DATA = None\n",
    "\n",
    "def set_initial_data(data_path):\n",
    "    global INITIAL_DATA\n",
    "    with open(data_path,'r') as f:\n",
    "        INITIAL_DATA = f.readlines()\n",
    "\n",
    "def generate_80_20_splits(num_splits,saving_name='CandC'):\n",
    "    if not INITIAL_DATA:\n",
    "        print(\"Please set initial data first!\")\n",
    "        return None\n",
    "\n",
    "    split_20_num = int(len(INITIAL_DATA) * 0.2)\n",
    "\n",
    "    for i in range(1,num_splits+1):\n",
    "        temp_data = deepcopy(INITIAL_DATA)\n",
    "        test_split = \"\"\n",
    "        for j in range(0,split_20_num):\n",
    "            test_selection = random.randint(0,len(temp_data)-1)\n",
    "            test_split+= temp_data[test_selection]\n",
    "            temp_data.pop(test_selection)\n",
    "\n",
    "        train_split = \"\".join(temp_data)\n",
    "        with open(\"{}-train{}.csv\".format(saving_name,i),'w') as f:\n",
    "            f.write(train_split)\n",
    "        with open(\"{}-test{}.csv\".format(saving_name,i),'w') as f:\n",
    "            f.write(test_split)\n",
    "\n",
    "def split_x_y(data):\n",
    "    '''\n",
    "    Takes in a list of rows where all columns except last one are xi's\n",
    "    and splits them into separate vectors\n",
    "    '''\n",
    "\n",
    "    split_data = []\n",
    "\n",
    "    for line in data:\n",
    "        temp_line = line.split(',')\n",
    "        y_vector = [float(temp_line[-1])]\n",
    "        temp_line.pop(-1)\n",
    "        x_vector = [float(i) for i in temp_line]\n",
    "\n",
    "        split_data.append((x_vector,y_vector))\n",
    "\n",
    "    return split_data\n",
    "\n",
    "def extract_x_from_split_data(data):\n",
    "    x_matrix = []\n",
    "    for row in data:\n",
    "        x_matrix.append(row[0])\n",
    "    return x_matrix\n",
    "\n",
    "def extract_y_from_split_data(data):\n",
    "    y_matrix = []\n",
    "    for row in data:\n",
    "        y_matrix.append(row[1])\n",
    "    return y_matrix\n",
    "\n",
    "\n",
    "def get_best_fit(x_matrix,y_matrix):\n",
    "    '''\n",
    "    Using the initialized data set this method finds the best fit for the data.\n",
    "    It uses the least squares regression method and returns an array containing\n",
    "    all the coefficients found with this method.\n",
    "    '''\n",
    "\n",
    "    mpps_x = np.linalg.pinv(np.array(x_matrix))\n",
    "\n",
    "    return np.matmul(mpps_x,np.array(y_matrix))\n",
    "\n",
    "\n",
    "def get_best_fit_l2(x_matrix,y_matrix,lambda_val):\n",
    "    x_matrix = np.array(x_matrix)\n",
    "    y_matrix = np.array(y_matrix)\n",
    "\n",
    "    output = np.matmul(x_matrix.transpose(),x_matrix)\n",
    "    output = output + (lambda_val*np.identity(len(x_matrix[0])))\n",
    "    output = np.linalg.inv(output)\n",
    "    output = np.matmul(np.matmul(output,x_matrix.transpose()),y_matrix)\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_predictions(coefficients,x_matrix):\n",
    "    predictions = []\n",
    "    for i in range(0,len(x_matrix)):\n",
    "        prediction = 0\n",
    "        for j in range(0,len(x_matrix[i])):\n",
    "            prediction += coefficients[j]*x_matrix[i][j]\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def get_mean_squared_error(target_values, predictions):\n",
    "    output = 0\n",
    "    num_predictions = len(predictions)\n",
    "\n",
    "    for i in range(0, num_predictions):\n",
    "        output += ((target_values[i] - predictions[i]) ** 2)\n",
    "\n",
    "    return (output / num_predictions)[0]\n",
    "\n",
    "\n",
    "def divide_interval(interval,num_divisions,end_point_inclusive=True):\n",
    "    '''\n",
    "    This method takes in a tuple that represents an interval and returns a list with the given number\n",
    "    of divisions. If the variable end_point_inclusive is set to true then the returned list has\n",
    "    the end_point_included\n",
    "    '''\n",
    "    output = [interval[0]]\n",
    "    if interval[0] < interval[1]:\n",
    "        delta = (interval[1] - interval[0])/num_divisions\n",
    "        point = interval[0] + delta\n",
    "        while point < interval[1]:\n",
    "            output.append(point)\n",
    "            point+=delta\n",
    "\n",
    "    if end_point_inclusive:\n",
    "        output.append(interval[1])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def find_min(dict_in):\n",
    "    '''\n",
    "    Takes a dictionary as input and returns the key and val with the smallest value\n",
    "    '''\n",
    "    if len(dict_in) < 1:\n",
    "        return None\n",
    "\n",
    "    min_key = list(dict_in.keys())[0]\n",
    "    min_value = dict_in[min_key]\n",
    "    output = [min_key,min_value]\n",
    "\n",
    "    for key,val in dict_in.items():\n",
    "        if val < output[1]:\n",
    "            output[0] = key\n",
    "            output[1] = val\n",
    "\n",
    "    return output\n",
    "\n",
    "def run_a():\n",
    "    init_data_path = 'Datasets/CrimeData/crime_data_updated_custom.csv'\n",
    "    split_data_name = 'Datasets/CrimeData/CandC'\n",
    "    num_splits = 5\n",
    "    lambda_divisions = 10\n",
    "\n",
    "    print(\"Initializing data from {}\".format(init_data_path))\n",
    "    set_initial_data(init_data_path)\n",
    "\n",
    "    print(\"Generating {} 80-20 splits with the initial data\".format(num_splits))\n",
    "    generate_80_20_splits(num_splits, saving_name=split_data_name)\n",
    "\n",
    "    lambda_values = divide_interval([0, 1], lambda_divisions, True)\n",
    "    mse_dict = {}\n",
    "    for l_val in lambda_values:\n",
    "        total_mse = 0\n",
    "        for i in range(1, num_splits + 1):\n",
    "            training_data = []\n",
    "            testing_data = []\n",
    "\n",
    "            with open(\"{}-train{}.csv\".format(split_data_name, i), 'r') as f:\n",
    "                training_data = f.readlines()\n",
    "\n",
    "            with open(\"{}-test{}.csv\".format(split_data_name, i), 'r') as f:\n",
    "                testing_data = f.readlines()\n",
    "\n",
    "            split_training_data = split_x_y(training_data)\n",
    "            split_testing_data = split_x_y(testing_data)\n",
    "\n",
    "            training_data_x = extract_x_from_split_data(split_training_data)\n",
    "            training_data_y = extract_y_from_split_data(split_training_data)\n",
    "            testing_data_x = extract_x_from_split_data(split_testing_data)\n",
    "            testing_data_y = extract_y_from_split_data(split_testing_data)\n",
    "\n",
    "            best_fit_coefficients = get_best_fit_l2(training_data_x, training_data_y, l_val)\n",
    "            predictions = get_predictions(best_fit_coefficients, testing_data_x)\n",
    "            mse = get_mean_squared_error(testing_data_y, predictions)\n",
    "\n",
    "            total_mse += mse\n",
    "\n",
    "        avg_mse = total_mse / num_splits\n",
    "        mse_dict[l_val] = avg_mse\n",
    "\n",
    "    min_mse = find_min(mse_dict)\n",
    "    print(\"The smallest MSE {} was found for {}\".format(min_mse[1], min_mse[0]))\n",
    "    plt.plot(mse_dict.keys(), mse_dict.values(), 'b-')\n",
    "    plt.xlabel(\"Lambda Values\")\n",
    "    plt.ylabel(\"MSE Values\")\n",
    "    plt.axis([0, 1, 0, 0.1])\n",
    "    plt.suptitle(\"M.S.E. Values for Different Lambdas\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing data columns with missing data over 0.2 of the total data\n",
      "Removing all rows with missing data\n",
      "Updated data saved to Datasets/CrimeData/crime_data_feature_reduced.csv\n",
      "Initializing data from Datasets/CrimeData/crime_data_feature_reduced.csv\n",
      "Generating 5 80-20 splits with the initial data\n",
      "The smallest MSE 0.018719129594838187 was found for 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "INITIAL_DATA = None\n",
    "\n",
    "def set_initial_data(data_path):\n",
    "    global INITIAL_DATA\n",
    "    with open(data_path,'r') as f:\n",
    "        INITIAL_DATA = f.readlines()\n",
    "\n",
    "def generate_80_20_splits(num_splits,saving_name='CandC'):\n",
    "    if not INITIAL_DATA:\n",
    "        print(\"Please set initial data first!\")\n",
    "        return None\n",
    "\n",
    "    split_20_num = int(len(INITIAL_DATA) * 0.2)\n",
    "\n",
    "    for i in range(1,num_splits+1):\n",
    "        temp_data = deepcopy(INITIAL_DATA)\n",
    "        test_split = \"\"\n",
    "        for j in range(0,split_20_num):\n",
    "            test_selection = random.randint(0,len(temp_data)-1)\n",
    "            test_split+= temp_data[test_selection]\n",
    "            temp_data.pop(test_selection)\n",
    "\n",
    "        train_split = \"\".join(temp_data)\n",
    "        with open(\"{}-train{}.csv\".format(saving_name,i),'w') as f:\n",
    "            f.write(train_split)\n",
    "        with open(\"{}-test{}.csv\".format(saving_name,i),'w') as f:\n",
    "            f.write(test_split)\n",
    "\n",
    "def split_x_y(data):\n",
    "    '''\n",
    "    Takes in a list of rows where all columns except last one are xi's\n",
    "    and splits them into separate vectors\n",
    "    '''\n",
    "\n",
    "    split_data = []\n",
    "\n",
    "    for line in data:\n",
    "        temp_line = line.split(',')\n",
    "        y_vector = [float(temp_line[-1])]\n",
    "        temp_line.pop(-1)\n",
    "        x_vector = [float(i) for i in temp_line]\n",
    "\n",
    "        split_data.append((x_vector,y_vector))\n",
    "\n",
    "    return split_data\n",
    "\n",
    "def extract_x_from_split_data(data):\n",
    "    x_matrix = []\n",
    "    for row in data:\n",
    "        x_matrix.append(row[0])\n",
    "    return x_matrix\n",
    "\n",
    "def extract_y_from_split_data(data):\n",
    "    y_matrix = []\n",
    "    for row in data:\n",
    "        y_matrix.append(row[1])\n",
    "    return y_matrix\n",
    "\n",
    "\n",
    "def get_best_fit(x_matrix,y_matrix):\n",
    "    '''\n",
    "    Using the initialized data set this method finds the best fit for the data.\n",
    "    It uses the least squares regression method and returns an array containing\n",
    "    all the coefficients found with this method.\n",
    "    '''\n",
    "\n",
    "    mpps_x = np.linalg.pinv(np.array(x_matrix))\n",
    "\n",
    "    return np.matmul(mpps_x,np.array(y_matrix))\n",
    "\n",
    "\n",
    "def get_best_fit_l2(x_matrix,y_matrix,lambda_val):\n",
    "    x_matrix = np.array(x_matrix)\n",
    "    y_matrix = np.array(y_matrix)\n",
    "\n",
    "    output = np.matmul(x_matrix.transpose(),x_matrix)\n",
    "    output = output + (lambda_val*np.identity(len(x_matrix[0])))\n",
    "    output = np.linalg.inv(output)\n",
    "    output = np.matmul(np.matmul(output,x_matrix.transpose()),y_matrix)\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_predictions(coefficients,x_matrix):\n",
    "    predictions = []\n",
    "    for i in range(0,len(x_matrix)):\n",
    "        prediction = 0\n",
    "        for j in range(0,len(x_matrix[i])):\n",
    "            prediction += coefficients[j]*x_matrix[i][j]\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def get_mean_squared_error(target_values, predictions):\n",
    "    output = 0\n",
    "    num_predictions = len(predictions)\n",
    "\n",
    "    for i in range(0, num_predictions):\n",
    "        output += ((target_values[i] - predictions[i]) ** 2)\n",
    "\n",
    "    return (output / num_predictions)[0]\n",
    "\n",
    "\n",
    "def divide_interval(interval,num_divisions,end_point_inclusive=True):\n",
    "    '''\n",
    "    This method takes in a tuple that represents an interval and returns a list with the given number\n",
    "    of divisions. If the variable end_point_inclusive is set to true then the returned list has\n",
    "    the end_point_included\n",
    "    '''\n",
    "    output = [interval[0]]\n",
    "    if interval[0] < interval[1]:\n",
    "        delta = (interval[1] - interval[0])/num_divisions\n",
    "        point = interval[0] + delta\n",
    "        while point < interval[1]:\n",
    "            output.append(point)\n",
    "            point+=delta\n",
    "\n",
    "    if end_point_inclusive:\n",
    "        output.append(interval[1])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def find_min(dict_in):\n",
    "    '''\n",
    "    Takes a dictionary as input and returns the key and val with the smallest value\n",
    "    '''\n",
    "    if len(dict_in) < 1:\n",
    "        return None\n",
    "\n",
    "    min_key = list(dict_in.keys())[0]\n",
    "    min_value = dict_in[min_key]\n",
    "    output = [min_key,min_value]\n",
    "\n",
    "    for key,val in dict_in.items():\n",
    "        if val < output[1]:\n",
    "            output[0] = key\n",
    "            output[1] = val\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def remove_missing_data_cols(data_path,removal_threshold=0.5):\n",
    "    '''\n",
    "    This method removes all variables that have instances greater that the removal threshold for missing data\n",
    "    and updates the rest of the missing data with median.\n",
    "    '''\n",
    "\n",
    "    with open(data_path,'r') as f:\n",
    "        lines=f.readlines()\n",
    "\n",
    "    missing_data_cols = {}\n",
    "    for i in range(0,len(lines[0].split(','))):\n",
    "        missing_data_cols[i] = 0\n",
    "\n",
    "    for line in lines:\n",
    "        temp_list = line.split(',')\n",
    "        for i in range(0,len(temp_list)):\n",
    "            if temp_list[i] == \"\":\n",
    "                missing_data_cols[i]+=1\n",
    "\n",
    "    total_features = len(lines)\n",
    "    removal_threshold = removal_threshold*total_features\n",
    "    output = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        temp_list=line.split(',')\n",
    "        formatted_list = []\n",
    "        for i in range(0,len(temp_list)):\n",
    "            if missing_data_cols[i] < removal_threshold:\n",
    "                formatted_list.append(temp_list[i])\n",
    "        output+= (',').join(formatted_list)\n",
    "\n",
    "    return output\n",
    "\n",
    "def remove_missing_data_rows(data_path):\n",
    "    '''\n",
    "    This method removes all rows that have any missing data\n",
    "    '''\n",
    "    output = \"\"\n",
    "    with open(data_path,'r') as f:\n",
    "        lines=f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        add_to_output = True\n",
    "        temp_list=line.split(',')\n",
    "        for i in range(0,len(temp_list)):\n",
    "            if temp_list[i] == \"\":\n",
    "                add_to_output=False\n",
    "                break\n",
    "        if add_to_output:\n",
    "            output+=line\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def run_a():\n",
    "    init_data_path = 'Datasets/CrimeData/crime_data_updated_custom.csv'\n",
    "    split_data_name = 'Datasets/CrimeData/CandC'\n",
    "    num_splits = 5\n",
    "    lambda_divisions = 10\n",
    "\n",
    "    print(\"Initializing data from {}\".format(init_data_path))\n",
    "    set_initial_data(init_data_path)\n",
    "\n",
    "    print(\"Generating {} 80-20 splits with the initial data\".format(num_splits))\n",
    "    generate_80_20_splits(num_splits, saving_name=split_data_name)\n",
    "\n",
    "    lambda_values = divide_interval([0, 1], lambda_divisions, True)\n",
    "    mse_dict = {}\n",
    "    for l_val in lambda_values:\n",
    "        total_mse = 0\n",
    "        for i in range(1, num_splits + 1):\n",
    "            training_data = []\n",
    "            testing_data = []\n",
    "\n",
    "            with open(\"{}-train{}.csv\".format(split_data_name, i), 'r') as f:\n",
    "                training_data = f.readlines()\n",
    "\n",
    "            with open(\"{}-test{}.csv\".format(split_data_name, i), 'r') as f:\n",
    "                testing_data = f.readlines()\n",
    "\n",
    "            split_training_data = split_x_y(training_data)\n",
    "            split_testing_data = split_x_y(testing_data)\n",
    "\n",
    "            training_data_x = extract_x_from_split_data(split_training_data)\n",
    "            training_data_y = extract_y_from_split_data(split_training_data)\n",
    "            testing_data_x = extract_x_from_split_data(split_testing_data)\n",
    "            testing_data_y = extract_y_from_split_data(split_testing_data)\n",
    "\n",
    "            best_fit_coefficients = get_best_fit_l2(training_data_x, training_data_y, l_val)\n",
    "            predictions = get_predictions(best_fit_coefficients, testing_data_x)\n",
    "            mse = get_mean_squared_error(testing_data_y, predictions)\n",
    "\n",
    "            total_mse += mse\n",
    "\n",
    "        avg_mse = total_mse / num_splits\n",
    "        mse_dict[l_val] = avg_mse\n",
    "\n",
    "    min_mse = find_min(mse_dict)\n",
    "    print(\"The smallest MSE {} was found for {}\".format(min_mse[1], min_mse[0]))\n",
    "    plt.plot(mse_dict.keys(), mse_dict.values(), 'b-')\n",
    "    plt.xlabel(\"Lambda Values\")\n",
    "    plt.ylabel(\"MSE Values\")\n",
    "    plt.axis([0, 1, 0, 0.1])\n",
    "    plt.suptitle(\"M.S.E. Values for Different Lambdas\")\n",
    "    plt.show()\n",
    "\n",
    "def run_c():\n",
    "    removal_threshold = 0.2\n",
    "    init_data_path = 'Datasets/CrimeData/crime_data_updated_custom.csv'\n",
    "    reduced_data_path = 'Datasets/CrimeData/crime_data_feature_reduced.csv'\n",
    "    split_data_name = 'Datasets/CrimeData/CandC'\n",
    "    num_splits = 5\n",
    "    lambda_divisions = 10\n",
    "\n",
    "    print(\"Removing data columns with missing data over {} of the total data\".format(removal_threshold))\n",
    "    refined_data = remove_missing_data_cols(data_path=init_data_path,removal_threshold=removal_threshold)\n",
    "    with open(reduced_data_path,'w') as f:\n",
    "        f.write(refined_data)\n",
    "    print(\"Removing all rows with missing data\")\n",
    "    refined_data = remove_missing_data_rows(reduced_data_path)\n",
    "    with open(reduced_data_path,'w') as f:\n",
    "        f.write(refined_data)\n",
    "    print(\"Updated data saved to {}\".format(reduced_data_path))\n",
    "\n",
    "    print(\"Initializing data from {}\".format(reduced_data_path))\n",
    "    set_initial_data(reduced_data_path)\n",
    "    print(\"Generating {} 80-20 splits with the initial data\".format(num_splits))\n",
    "    generate_80_20_splits(num_splits, saving_name=split_data_name)\n",
    "\n",
    "    lambda_values = divide_interval([0, 1], lambda_divisions, True)\n",
    "    mse_dict = {}\n",
    "    for l_val in lambda_values:\n",
    "        total_mse = 0\n",
    "        for i in range(1, num_splits + 1):\n",
    "            training_data = []\n",
    "            testing_data = []\n",
    "\n",
    "            with open(\"{}-train{}.csv\".format(split_data_name, i), 'r') as f:\n",
    "                training_data = f.readlines()\n",
    "\n",
    "            with open(\"{}-test{}.csv\".format(split_data_name, i), 'r') as f:\n",
    "                testing_data = f.readlines()\n",
    "\n",
    "            split_training_data = split_x_y(training_data)\n",
    "            split_testing_data = split_x_y(testing_data)\n",
    "\n",
    "            training_data_x = extract_x_from_split_data(split_training_data)\n",
    "            training_data_y = extract_y_from_split_data(split_training_data)\n",
    "            testing_data_x = extract_x_from_split_data(split_testing_data)\n",
    "            testing_data_y = extract_y_from_split_data(split_testing_data)\n",
    "\n",
    "            best_fit_coefficients = get_best_fit_l2(training_data_x, training_data_y, l_val)\n",
    "            predictions = get_predictions(best_fit_coefficients, testing_data_x)\n",
    "            mse = get_mean_squared_error(testing_data_y, predictions)\n",
    "\n",
    "            total_mse += mse\n",
    "\n",
    "        avg_mse = total_mse / num_splits\n",
    "        mse_dict[l_val] = avg_mse\n",
    "\n",
    "    min_mse = find_min(mse_dict)\n",
    "    print(\"The smallest MSE {} was found for {}\".format(min_mse[1], min_mse[0]))\n",
    "    plt.plot(mse_dict.keys(), mse_dict.values(), 'b-')\n",
    "    plt.xlabel(\"Lambda Values\")\n",
    "    plt.ylabel(\"MSE Values\")\n",
    "    plt.axis([0, 1, 0, 0.1])\n",
    "    plt.suptitle(\"M.S.E. Values for Different Lambdas\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
